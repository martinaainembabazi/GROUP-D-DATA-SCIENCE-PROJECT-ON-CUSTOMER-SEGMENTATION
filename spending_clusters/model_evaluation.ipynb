{
 "cells": [
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Model Evaluation for Spending Cluster Prediction\n",
    "# Compares Logistic Regression and Random Forest across multiple feature sets\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold, GridSearchCV, cross_val_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "import joblib\n",
    "import os\n"
   ],
   "id": "61938a8b8092618f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# 1. Load Data\n",
    "\n",
    "df = pd.read_csv('../featured_customer_segmentation_with_clusters.csv')\n"
   ],
   "id": "c9ee55f82c12fcbd",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# 2. Correlation Plot Visualization\n",
    "corr_features = [\n",
    "    'Income', 'Age', 'Education', 'Total_Dependents', 'Teenhome', 'Kidhome',\n",
    "    'Marital_Together', 'Marital_Single', 'Marital_Divorced', 'Marital_Widow', 'Marital_Married',\n",
    "    'weighted_total_dependents', 'Spending_Cluster'\n",
    "]\n",
    "if 'weighted_total_dependents' not in df.columns:\n",
    "    df['weighted_total_dependents'] = df['Kidhome'] * 2 + df['Teenhome']\n",
    "\n",
    "corr = df[corr_features].corr()\n",
    "mask = np.triu(np.ones_like(corr, dtype=bool))\n",
    "plt.figure(figsize=(14, 12))\n",
    "sns.heatmap(\n",
    "    corr,\n",
    "    mask=mask,\n",
    "    annot=True,\n",
    "    fmt='.2f',\n",
    "    linewidths=.5,\n",
    "    square=True,\n",
    "    cbar_kws={'shrink': .8, 'label': 'Pearson ρ'},\n",
    "    vmin=-1, vmax=1,\n",
    "    center=0,\n",
    "    cmap='vlag'\n",
    ")\n",
    "plt.title('Feature Correlation Matrix (Lower Triangle)', fontsize=18, pad=20)\n",
    "plt.xticks(rotation=45, ha='right', fontsize=12)\n",
    "plt.yticks(rotation=0, fontsize=12)\n",
    "plt.gcf().text(\n",
    "    0.01, 0.01,\n",
    "    \"■ Strong positive (> 0.7)\\n■ Strong negative (< -0.7)\\n■ Near zero: weak/no linear relation\",\n",
    "    fontsize=10,\n",
    "    bbox=dict(facecolor='white', alpha=0.8)\n",
    ")\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ],
   "id": "cd92cfe2aefa2ad1",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# 3. Feature Sets\n",
    "feature_sets = {\n",
    "    'v1': ['Income', 'Age', 'Education', 'Total_Dependents'],\n",
    "    'v2': ['Income', 'Age', 'Education', 'Teenhome', 'Kidhome'],\n",
    "    'v3': ['Income', 'Age', 'Education',\n",
    "           'Marital_Together', 'Marital_Single', 'Marital_Divorced', 'Marital_Widow', 'Marital_Married',\n",
    "           'Total_Dependents'],\n",
    "    'v4': ['Income', 'Age', 'Education', 'Kidhome'],\n",
    "    'v5': ['Income', 'Age', 'Education', 'weighted_total_dependents']\n",
    "}\n",
    "\n",
    "y = df['Spending_Cluster']\n"
   ],
   "id": "f64db216bbcbf232",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# 4. Model Evaluation Loop\n",
    "param_grid_rf = {\n",
    "    'n_estimators': [100, 300],\n",
    "    'max_depth': [None, 10],\n",
    "    'min_samples_split': [2, 5],\n",
    "    'min_samples_leaf': [1, 2]\n",
    "}\n",
    "param_grid_lr = {\n",
    "    'C': [0.01, 0.1, 1, 10],\n",
    "    'penalty': ['l2'],\n",
    "    'solver': ['lbfgs', 'liblinear']\n",
    "}\n",
    "kf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "results = []\n",
    "\n",
    "for variant, feats in feature_sets.items():\n",
    "    print(f\"\\n=== Variant: {variant} | Features: {feats} ===\")\n",
    "    X = df[feats]\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, test_size=0.2, random_state=42)\n",
    "    scaler = StandardScaler()\n",
    "    X_train_s = scaler.fit_transform(X_train)\n",
    "    X_test_s = scaler.transform(X_test)\n",
    "\n",
    "    # Logistic Regression\n",
    "    grid_lr = GridSearchCV(LogisticRegression(max_iter=1000), param_grid_lr, cv=kf, scoring='accuracy', n_jobs=-1)\n",
    "    grid_lr.fit(X_train_s, y_train)\n",
    "    best_lr = grid_lr.best_estimator_\n",
    "    lr_preds = best_lr.predict(X_test_s)\n",
    "    lr_acc = accuracy_score(y_test, lr_preds) * 100\n",
    "    lr_cv_acc = cross_val_score(best_lr, scaler.fit_transform(X), y, scoring='accuracy', cv=kf)\n",
    "    print(f\"LogisticRegression CV Accuracy: {lr_cv_acc.mean() * 100:.2f}% ± {lr_cv_acc.std() * 100:.2f}%\")\n",
    "    print(f\"LogisticRegression Test Accuracy: {lr_acc:.2f}%\")\n",
    "    print(\"Best LR Params:\", grid_lr.best_params_)\n",
    "    print(\"Confusion Matrix (LR):\\n\", confusion_matrix(y_test, lr_preds))\n",
    "\n",
    "    # Random Forest\n",
    "    grid_rf = GridSearchCV(RandomForestClassifier(random_state=42), param_grid_rf, cv=kf, scoring='accuracy', n_jobs=-1)\n",
    "    grid_rf.fit(X_train_s, y_train)\n",
    "    best_rf = grid_rf.best_estimator_\n",
    "    rf_preds = best_rf.predict(X_test_s)\n",
    "    rf_acc = accuracy_score(y_test, rf_preds) * 100\n",
    "    rf_cv_acc = cross_val_score(best_rf, scaler.fit_transform(X), y, scoring='accuracy', cv=kf)\n",
    "    print(f\"RandomForest CV Accuracy: {rf_cv_acc.mean() * 100:.2f}% ± {rf_cv_acc.std() * 100:.2f}%\")\n",
    "    print(f\"RandomForest Test Accuracy: {rf_acc:.2f}%\")\n",
    "    print(\"Best RF Params:\", grid_rf.best_params_)\n",
    "    print(\"Confusion Matrix (RF):\\n\", confusion_matrix(y_test, rf_preds))\n",
    "\n",
    "    results.append({\n",
    "        'Variant': variant,\n",
    "        'LR_CV_Acc': lr_cv_acc.mean() * 100,\n",
    "        'LR_Test_Acc': lr_acc,\n",
    "        'RF_CV_Acc': rf_cv_acc.mean() * 100,\n",
    "        'RF_Test_Acc': rf_acc\n",
    "    })\n"
   ],
   "id": "3d9c628d721833cf",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# 5. Results Summary\n",
    "results_df = pd.DataFrame(results)\n",
    "print(\"\\n=== Summary of All Variants ===\")\n",
    "print(results_df)\n"
   ],
   "id": "8431aa3e60142f2a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# 6. Save Best Model (example: best RF from v3)\n",
    "best_variant = 'v3'\n",
    "X = df[feature_sets[best_variant]]\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "best_rf = RandomForestClassifier(**grid_rf.best_params_, random_state=42)\n",
    "best_rf.fit(X_scaled, y)\n",
    "\n",
    "os.makedirs('model', exist_ok=True)\n",
    "joblib.dump(best_rf, 'model/spending_rf_v3_model.joblib')\n",
    "joblib.dump(scaler, 'model/spending_scaler_v3.joblib')\n",
    "\n",
    "print(f\"Best model and scaler for {best_variant} saved.\")\n"
   ],
   "id": "4d2ebe0fb214fdc",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": " # metat",
   "id": "d8ffb4c67fe5163",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
