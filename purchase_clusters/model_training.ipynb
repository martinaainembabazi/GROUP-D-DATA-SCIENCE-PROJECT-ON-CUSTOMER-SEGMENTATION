{
 "cells": [
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold, GridSearchCV, KFold\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, ConfusionMatrixDisplay\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import json\n",
    "# kfold\n"
   ],
   "id": "226124fefa2dd5d3",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "df = pd.read_csv('../featured_customer_segmentation_with_clusters.csv')\n",
    "# 1. Compute your correlations\n",
    "features = [\n",
    "    'Income', 'Age', 'Total_Dependents', 'Tenure_Days',\n",
    "    'Teenhome', 'Kidhome', 'Education',\n",
    "    'Marital_Together', 'Marital_Single', 'Marital_Divorced', 'Marital_Widow',\n",
    "    'PurchaseCluster'\n",
    "]\n",
    "corr = df[features].corr()\n",
    "\n",
    "# 2. Create a mask for the upper triangle\n",
    "mask = np.triu(np.ones_like(corr, dtype=bool))\n",
    "\n",
    "# 3. Plot\n",
    "plt.figure(figsize=(14, 12))\n",
    "sns.heatmap(\n",
    "    corr,\n",
    "    mask=mask,\n",
    "    annot=True,\n",
    "    fmt='.2f',\n",
    "    linewidths=.5,\n",
    "    square=True,\n",
    "    cbar_kws={'shrink': .8, 'label': 'Pearson ρ'},\n",
    "    vmin=-1, vmax=1,\n",
    "    center=0,\n",
    "    cmap='vlag'  # a red‑to‑blue diverging map; you could switch back to 'coolwarm' if you prefer\n",
    ")\n",
    "\n",
    "# 4. Styling\n",
    "plt.title('Feature Correlation Matrix (Lower Triangle)', fontsize=18, pad=20)\n",
    "plt.xticks(rotation=45, ha='right', fontsize=12)\n",
    "plt.yticks(rotation=0, fontsize=12)\n",
    "\n",
    "# 5. Interpretation legend\n",
    "plt.gcf().text(\n",
    "    0.01, 0.01,\n",
    "    \"■ Strong positive (> 0.7)\\n■ Strong negative (< -0.7)\\n■ Near zero: weak/no linear relation\",\n",
    "    fontsize=10,\n",
    "    bbox=dict(facecolor='white', alpha=0.8)\n",
    ")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(corr)\n"
   ],
   "id": "7ddedd8a4f07123d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "# -- Compute derived feature --\n",
    "df['weighted_total_dependents'] = df['Kidhome'] * 2 + df['Teenhome']\n",
    "\n",
    "# -- Feature variants --\n",
    "feature_sets = {\n",
    "    'v1': ['Income', 'Age', 'Education', 'Total_Dependents'],\n",
    "    'v2': ['Income', 'Age', 'Education', 'Teenhome', 'Kidhome'],\n",
    "    'v3': ['Income', 'Age', 'Education',\n",
    "           'Marital_Together', 'Marital_Single', 'Marital_Divorced', 'Marital_Widow',\n",
    "           'Total_Dependents', 'Teenhome', 'Kidhome'],\n",
    "    'v4': ['Income', 'Age', 'Education', 'Kidhome'],\n",
    "    'v5': ['Income', 'Age', 'Education', 'weighted_total_dependents']\n",
    "}\n",
    "\n",
    "y = df['PurchaseCluster']\n",
    "\n",
    "# -- Hyperparameter grid for RF --\n",
    "param_grid_rf = {\n",
    "    'n_estimators': [100, 300],\n",
    "    'max_depth': [None, 10],\n",
    "    'min_samples_split': [2, 5],\n",
    "    'min_samples_leaf': [1, 2]\n",
    "}\n",
    "\n",
    "# -- Cross-validation setup --\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# -- Storage for summary results --\n",
    "summary = []\n",
    "\n",
    "# -- Explore both Logistic Regression and RF Classifier for each variant --\n",
    "for variant, feats in feature_sets.items():\n",
    "    print(f\"\\n=== Variant: {variant} | Features: {feats} ===\")\n",
    "    X = df[feats]\n",
    "\n",
    "    # Split\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "    # Scale\n",
    "    scaler = StandardScaler()\n",
    "    X_train_s = scaler.fit_transform(X_train)\n",
    "    X_test_s = scaler.transform(X_test)\n",
    "\n",
    "    # -- Logistic Regression --\n",
    "    lr = LogisticRegression(max_iter=1000)\n",
    "    # CV Accuracy\n",
    "    lr_cv_acc = cross_val_score(lr, scaler.fit_transform(X), y, scoring='accuracy', cv=kf)\n",
    "    print(f\"LogisticRegression CV Accuracy: {lr_cv_acc.mean() * 100:.2f}% ± {lr_cv_acc.std() * 100:.2f}%\")\n",
    "    # Test-set Accuracy\n",
    "    lr.fit(X_train_s, y_train)\n",
    "    lr_preds = lr.predict(X_test_s)\n",
    "    lr_acc = accuracy_score(y_test, lr_preds) * 100\n",
    "    print(f\"LogisticRegression Test Accuracy: {lr_acc:.2f}%\")\n",
    "\n",
    "    # -- Random Forest Classifier --\n",
    "    rf = RandomForestClassifier(random_state=42)\n",
    "    rf_cv_acc = cross_val_score(rf, scaler.fit_transform(X), y, scoring='accuracy', cv=kf)\n",
    "    print(f\"RandomForest CV Accuracy: {rf_cv_acc.mean() * 100:.2f}% ± {rf_cv_acc.std() * 100:.2f}%\")\n",
    "\n",
    "    grid = GridSearchCV(rf, param_grid_rf, cv=5, scoring='accuracy', n_jobs=-1)\n",
    "    grid.fit(X_train_s, y_train)\n",
    "    best_rf = grid.best_estimator_\n",
    "    rf_preds = best_rf.predict(X_test_s)\n",
    "    rf_acc = accuracy_score(y_test, rf_preds) * 100\n",
    "    print(f\"RandomForest Test Accuracy: {rf_acc:.2f}%\")\n",
    "    print(f\"Best RF Params: {grid.best_params_}\")\n",
    "\n",
    "    # Save summary\n",
    "    summary.append({\n",
    "        'Variant': variant,\n",
    "        'LR_CV_Acc': lr_cv_acc.mean() * 100,\n",
    "        'LR_Test_Acc': lr_acc,\n",
    "        'RF_CV_Acc': rf_cv_acc.mean() * 100,\n",
    "        'RF_Test_Acc': rf_acc\n",
    "    })\n",
    "\n",
    "# -- Summary DataFrame --\n",
    "summary_df = pd.DataFrame(summary)\n",
    "print(\"\\n=== Summary of All Variants (Percentage Accuracy) ===\")\n",
    "print(summary_df)\n"
   ],
   "id": "fc3d207afe6f5cac",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "\n",
    "# -- Log-transform skewed Income --\n",
    "df['Income'] = np.log1p(df['Income'])\n",
    "\n",
    "# -- Weighted dependents feature --\n",
    "df['weighted_total_dependents'] = df['Kidhome'] * 2 + df['Teenhome']\n",
    "\n",
    "# -- Feature sets (Education is ordinal) --\n",
    "feature_sets = {\n",
    "    'v1': ['Income', 'Age', 'Education', 'Total_Dependents'],\n",
    "    'v2': ['Income', 'Age', 'Education', 'Teenhome', 'Kidhome'],\n",
    "    'v3': ['Income', 'Age', 'Education',\n",
    "           'Marital_Together', 'Marital_Single', 'Marital_Divorced', 'Marital_Widow', 'Marital_Married',\n",
    "           'Total_Dependents'],\n",
    "    'v4': ['Income', 'Age', 'Education', 'Kidhome'],\n",
    "    'v5': ['Income', 'Age', 'Education', 'weighted_total_dependents']\n",
    "}\n",
    "\n",
    "y = df['PurchaseCluster']\n",
    "\n",
    "# -- Hyperparameter grids --\n",
    "param_grid_rf = {\n",
    "    'n_estimators': [100, 300],\n",
    "    'max_depth': [None, 10],\n",
    "    'min_samples_split': [2, 5],\n",
    "    'min_samples_leaf': [1, 2]\n",
    "}\n",
    "\n",
    "param_grid_lr = {\n",
    "    'C': [0.01, 0.1, 1, 10],\n",
    "    'penalty': ['l2'],\n",
    "    'solver': ['lbfgs', 'liblinear']\n",
    "}\n",
    "\n",
    "# -- Stratified Cross-validation --\n",
    "kf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "summary = []\n",
    "\n",
    "for variant, feats in feature_sets.items():\n",
    "    print(f\"\\n=== Variant: {variant} | Features: {feats} ===\")\n",
    "    X = df[feats]\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, test_size=0.2, random_state=42)\n",
    "\n",
    "    # Scaling\n",
    "    scaler = StandardScaler()\n",
    "    X_train_s = scaler.fit_transform(X_train)\n",
    "    X_test_s = scaler.transform(X_test)\n",
    "\n",
    "    # -- Logistic Regression (tuned) --\n",
    "    grid_lr = GridSearchCV(LogisticRegression(max_iter=1000), param_grid_lr, cv=kf, scoring='accuracy', n_jobs=-1)\n",
    "    grid_lr.fit(X_train_s, y_train)\n",
    "    best_lr = grid_lr.best_estimator_\n",
    "    lr_preds = best_lr.predict(X_test_s)\n",
    "    lr_acc = accuracy_score(y_test, lr_preds) * 100\n",
    "    lr_cv_acc = cross_val_score(best_lr, scaler.fit_transform(X), y, scoring='accuracy', cv=kf)\n",
    "    print(f\"LogisticRegression CV Accuracy: {lr_cv_acc.mean() * 100:.2f}% ± {lr_cv_acc.std() * 100:.2f}%\")\n",
    "    print(f\"LogisticRegression Test Accuracy: {lr_acc:.2f}%\")\n",
    "    print(\"Best LR Params:\", grid_lr.best_params_)\n",
    "\n",
    "    # Confusion matrix for LR\n",
    "    print(\"confusion_matrix for lr:\")\n",
    "\n",
    "    print(confusion_matrix(y_test, lr_preds))\n",
    "\n",
    "    # -- Random Forest Classifier (tuned) --\n",
    "    grid_rf = GridSearchCV(RandomForestClassifier(random_state=42), param_grid_rf, cv=kf, scoring='accuracy', n_jobs=-1)\n",
    "    grid_rf.fit(X_train_s, y_train)\n",
    "    best_rf = grid_rf.best_estimator_\n",
    "    rf_preds = best_rf.predict(X_test_s)\n",
    "    rf_acc = accuracy_score(y_test, rf_preds) * 100\n",
    "    rf_cv_acc = cross_val_score(best_rf, scaler.fit_transform(X), y, scoring='accuracy', cv=kf)\n",
    "    print(f\"RandomForest CV Accuracy: {rf_cv_acc.mean() * 100:.2f}% ± {rf_cv_acc.std() * 100:.2f}%\")\n",
    "    print(f\"RandomForest Test Accuracy: {rf_acc:.2f}%\")\n",
    "    print(\"Best RF Params:\", grid_rf.best_params_)\n",
    "\n",
    "    print(\"confusion_matrix for rf:\")\n",
    "    print(confusion_matrix(y_test, rf_preds))\n",
    "\n",
    "    # -- Store results --\n",
    "    summary.append({\n",
    "        'Variant': variant,\n",
    "        'LR_CV_Acc': lr_cv_acc.mean() * 100,\n",
    "        'LR_Test_Acc': lr_acc,\n",
    "        'RF_CV_Acc': rf_cv_acc.mean() * 100,\n",
    "        'RF_Test_Acc': rf_acc\n",
    "    })\n",
    "\n",
    "# -- Summary Results --\n",
    "summary_df = pd.DataFrame(summary)\n",
    "print(\"\\n=== Summary of All Variants (Final) ===\")\n",
    "print(summary_df)\n"
   ],
   "id": "21fbf20e9ea0e1",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "\n",
    "# -- Log-transform skewed Income --\n",
    "df['Income'] = np.log1p(df['Income'])\n",
    "\n",
    "# -- Weighted dependents feature --\n",
    "df['weighted_total_dependents'] = df['Kidhome'] * 2 + df['Teenhome']\n",
    "\n",
    "# -- v3 Feature set --\n",
    "features_v3 = [\n",
    "    'Income', 'Age', 'Education',\n",
    "    'Marital_Together', 'Marital_Single', 'Marital_Divorced', 'Marital_Widow', 'Marital_Married',\n",
    "    'Total_Dependents'\n",
    "]\n",
    "X = df[features_v3]\n",
    "y = df['PurchaseCluster']\n",
    "\n",
    "# -- Hyperparameter grid for Random Forest --\n",
    "param_grid_rf = {\n",
    "    'n_estimators': [100, 300],\n",
    "    'max_depth': [None, 10],\n",
    "    'min_samples_split': [2, 5],\n",
    "    'min_samples_leaf': [1, 2]\n",
    "}\n",
    "\n",
    "# -- Stratified Cross-validation --\n",
    "kf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# -- Train/Test Split --\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, test_size=0.2, random_state=42)\n",
    "\n",
    "# -- Scaling --\n",
    "scaler = StandardScaler()\n",
    "X_train_s = scaler.fit_transform(X_train)\n",
    "X_test_s = scaler.transform(X_test)\n",
    "\n",
    "# -- Random Forest Classifier (tuned) --\n",
    "grid_rf = GridSearchCV(RandomForestClassifier(random_state=42), param_grid_rf, cv=kf, scoring='accuracy', n_jobs=-1)\n",
    "grid_rf.fit(X_train_s, y_train)\n",
    "best_rf = grid_rf.best_estimator_\n",
    "rf_preds = best_rf.predict(X_test_s)\n",
    "rf_acc = accuracy_score(y_test, rf_preds) * 100\n",
    "rf_cv_acc = cross_val_score(best_rf, scaler.fit_transform(X), y, scoring='accuracy', cv=kf)\n",
    "\n",
    "print(f\"RandomForest CV Accuracy: {rf_cv_acc.mean() * 100:.2f}% ± {rf_cv_acc.std() * 100:.2f}%\")\n",
    "print(f\"RandomForest Test Accuracy: {rf_acc:.2f}%\")\n",
    "print(\"Best RF Params:\", grid_rf.best_params_)\n",
    "\n",
    "print(\"confusion_matrix for rf:\")\n",
    "print(confusion_matrix(y_test, rf_preds))\n",
    "\n",
    "# -- Summary Results --\n",
    "summary = [{\n",
    "    'Variant': 'v3',\n",
    "    'RF_CV_Acc': rf_cv_acc.mean() * 100,\n",
    "    'RF_Test_Acc': rf_acc\n",
    "}]\n",
    "summary_df = pd.DataFrame(summary)\n",
    "print(\"\\n=== Summary for v3 Random Forest ===\")\n",
    "print(summary_df)\n",
    "\n"
   ],
   "id": "1751c426376f6134",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import os, joblib\n",
    "path = f\"{os.getcwd()}/model\"\n",
    "os.makedirs(path, exist_ok=True)\n",
    "\n",
    "# Save the trained model\n",
    "joblib.dump(best_rf, f\"{path}/random_forest_v3_model.joblib\")\n",
    "\n",
    "# Save the scaler\n",
    "joblib.dump(scaler, f\"{path}/scaler_v3.joblib\")\n",
    "\n",
    "metadata = {\n",
    "    'features': features_v3,\n",
    "    'target': 'PurchaseCluster',\n",
    "    'model_type': 'Random Forest',\n",
    "    'hyperparameters': grid_rf.best_params_,\n",
    "    'accuracy': rf_acc,\n",
    "    'cv_accuracy': rf_cv_acc.mean() * 100\n",
    "}\n",
    "# Save metadata\n",
    "with open(f\"{path}/model_metadata_v3.json\", 'w') as f:\n",
    "\n",
    "    json.dump(metadata, f, indent=4)\n"
   ],
   "id": "71f5da08d531b0f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "84effbfc7cefbc81",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
