{ "cells": [  {   "cell_type": "markdown",   "id": "192575cb4bcb48a0",   "metadata": {},   "source": [    "# Customer Purchase Behavior Clustering\n",    "- This notebook performs clustering on customer purchase behavior data to identify distinct purchase patterns."   ]  },  {   "cell_type": "code",   "id": "89acadd6ae2b973c",   "metadata": {},   "source": [    "import pandas as pd\n",    "import matplotlib.pyplot as plt\n",    "from sklearn.cluster import KMeans\n",    "from sklearn.preprocessing import StandardScaler\n",    "import numpy as np\n",    "\n",    "from sklearn.ensemble import RandomForestClassifier\n",    "from sklearn.linear_model import LogisticRegression\n",    "\n",    "import seaborn as sns\n",    "# pca\n",    "from sklearn.decomposition import PCA\n",    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV, KFold\n",    "from sklearn.linear_model import LinearRegression\n",    "from sklearn.ensemble import RandomForestRegressor\n",    "from sklearn.metrics import mean_squared_error, r2_score, accuracy_score\n",    "\n"   ],   "outputs": [],   "execution_count": null  },  {   "cell_type": "code",   "id": "7d4ae0dc58f2d959",   "metadata": {},   "source": [    "\n",    "purchase_behavior_features = [\n",    "    'NumDealsPurchases',  # Promo purchases\n",    "    'NumWebPurchases',  # Online\n",    "    'NumCatalogPurchases',  # Catalog\n",    "    'NumStorePurchases',  # In-store\n",    "    'NumWebVisitsMonth'  # Website visits (optional: inverse indicator)\n",    "]\n",    "\n",    "df = pd.read_csv('featured_customer_segmentation.csv')\n",    "# Display the first few rows of the DataFrame\n",    "print(\"DataFrame Head:\")\n",    "print(df.head())"   ],   "outputs": [],   "execution_count": null  },  {   "cell_type": "code",   "id": "fc8a8ea5fa924c41",   "metadata": {},   "source": [    "scaler = StandardScaler()\n",    "X_behavior = scaler.fit_transform(df[purchase_behavior_features])\n",    "\n",    "inertia = []\n",    "for k in range(1, 10):\n",    "    kmeans = KMeans(n_clusters=k, random_state=42)\n",    "    kmeans.fit(X_behavior)\n",    "    inertia.append(kmeans.inertia_)\n",    "\n",    "print(inertia)\n",    "\n",    "plt.plot(range(1, 10), inertia, marker='o')\n",    "plt.xlabel('Number of Clusters')\n",    "plt.ylabel('Inertia')\n",    "plt.title('Elbow Method for purchase Pattern Clustering')\n",    "plt.grid(True)\n",    "plt.show()\n"   ],   "outputs": [],   "execution_count": null  },  {   "cell_type": "code",   "id": "437e92a25594c99f",   "metadata": {},   "source": [    "kmeans = KMeans(n_clusters=3, random_state=42)\n",    "df['PurchaseCluster'] = kmeans.fit_predict(X_behavior)\n",    "\n",    "# Print cluster distributions \n",    "cluster_counts = df['PurchaseCluster'].value_counts().sort_index()\n",    "print(\"\\nNumber of customers in each cluster:\")\n",    "print(cluster_counts)\n",    "print(\"\\nPercentage of customers in each cluster:\")\n",    "print((cluster_counts / len(df) * 100).round(2), \"%\")\n",    "\n",    "# Plot distribution\n",    "plt.figure(figsize=(10, 6))\n",    "cluster_counts.plot(kind='bar')\n",    "plt.title('Distribution of Customers Across Clusters')\n",    "plt.xlabel('Cluster')\n",    "plt.ylabel('Number of Customers')\n",    "plt.xticks(rotation=0)\n",    "for i, v in enumerate(cluster_counts):\n",    "    plt.text(i, v, str(v), ha='center', va='bottom')\n",    "plt.tight_layout()\n",    "plt.show()\n"   ],   "outputs": [],   "execution_count": null  },  {   "cell_type": "code",   "id": "5d79216edbde2bd5",   "metadata": {},   "source": [    "\n",    "# Organize features by category\n",    "feature_categories = {\n",    "    'Demographics': [ 'Education', 'Total_Dependents',\n",    "                    'Teenhome', 'Kidhome'],\n",    "    'Expenses': [ 'MntWines', 'MntFruits', 'MntMeatProducts',\n",    "                'MntFishProducts', 'MntSweetProducts', 'MntGoldProds'],\n",    "    'Purchase Channels': ['NumWebPurchases', 'NumStorePurchases', 'NumCatalogPurchases',\n",    "                         'NumDealsPurchases','NumWebVisitsMonth'],\n",    "    'cash': ['Total_Spending','Income'],\n",    "    'channel_ratios': ['Web_Ratio', 'Store_Ratio', 'Catalog_Ratio', 'Deals_Ratio'],\n",    "    'Expense_ratios': ['Wine_Ratio', 'Fruit_Ratio', 'Meat_Ratio',\n",    "                         'Fish_Ratio', 'Sweet_Ratio', 'Gold_Ratio']\n",    "}\n",    "\n",    "# Color palettes for each category\n",    "color_palettes = {\n",    "     'cash': 'Purples'  ,\n",    "    'Demographics': 'Blues',\n",    "\n",    "    'Purchase Channels': 'Oranges',\n",    "\n",    "    'channel_ratios': 'Reds',\n",    "    'Expenses': 'Greens',\n",    "    'Expense_ratios': 'Greys'\n",    "\n",    "}\n",    "\n",    "\n",    "# Plot each category\n",    "for category_name, features in feature_categories.items():\n",    "    # Calculate means by cluster\n",    "    cluster_means = df.groupby('PurchaseCluster')[features].mean().round(2)\n",    "    print(f\"\\n{category_name.upper()} MEANS BY CLUSTER:\")\n",    "    print(cluster_means)\n",    "\n",    "    # Create figure with appropriate size based on feature count\n",    "    fig_width = min(14, max(10, len(features)))\n",    "    fig, ax = plt.subplots(figsize=(fig_width, 8))\n",    "\n",    "    # Plot the transposed data for better visualization\n",    "    cluster_means.T.plot(kind='bar', ax=ax, colormap=color_palettes[category_name])\n",    "\n",    "    # Add styling\n",    "    plt.title(f'{category_name} by Purchase Cluster', fontsize=16, pad=20)\n",    "    plt.ylabel('Average Value', fontsize=12)\n",    "    plt.xlabel('Feature', fontsize=12)\n",    "    plt.xticks(rotation=45, ha='right')\n",    "    plt.legend(title='Cluster', fontsize=10)\n",    "    plt.grid(axis='y', linestyle='--', alpha=0.3)\n",    "\n",    "    # Add value labels on bars\n",    "    for container in ax.containers:\n",    "        ax.bar_label(container, fmt='%.1f', padding=3, fontsize=8)\n",    "\n",    "    plt.tight_layout()\n",    "    plt.show()\n",    "\n"   ],   "outputs": [],   "execution_count": null  },  {   "cell_type": "code",   "id": "fcb4d3467c2802e2",   "metadata": {},   "source": [    "# Create radar chart comparing key metrics across clusters\n",    "# Select representative metrics from each category\n",    "key_metrics = ['Age', 'Income', 'Education', 'Total_Spending',\n",    "              'NumWebPurchases' ,'NumWebVisitsMonth','NumStorePurchases', 'NumCatalogPurchases',]\n",    "\n",    "# Get data and normalize for radar chart\n",    "radar_data = df.groupby('PurchaseCluster')[key_metrics].mean()\n",    "\n",    "# Normalize each metric to 0-100 scale\n",    "for col in radar_data.columns:\n",    "    radar_data[col] = 100 * radar_data[col] / radar_data[col].max()\n",    "\n",    "# Set up radar chart\n",    "angles = np.linspace(0, 2*np.pi, len(key_metrics), endpoint=False).tolist()\n",    "angles += angles[:1]  # Close the circle\n",    "\n",    "fig, ax = plt.subplots(figsize=(10, 10), subplot_kw=dict(polar=True))\n",    "\n",    "# Plot each cluster\n",    "for cluster in radar_data.index:\n",    "    values = radar_data.loc[cluster].tolist()\n",    "    values += values[:1]  # Close the polygon\n",    "\n",    "    ax.plot(angles, values, linewidth=2, label=f'Cluster {cluster}')\n",    "    ax.fill(angles, values, alpha=0.1)\n",    "\n",    "# Add labels\n",    "labels = [x.replace('_', ' ') for x in key_metrics]\n",    "labels += [labels[0]]  # Complete the circle of labels\n",    "ax.set_xticks(angles)\n",    "ax.set_xticklabels(labels)\n",    "\n",    "# Customize chart\n",    "ax.set_title('Key Metrics by Purchase Cluster (Normalized)', size=16, pad=20)\n",    "ax.grid(True)\n",    "ax.legend(loc='upper right', bbox_to_anchor=(1.3, 1.0))\n",    "\n",    "plt.tight_layout()\n",    "plt.show()"   ],   "outputs": [],   "execution_count": null  },  {   "cell_type": "code",   "id": "74497c36e2e2bc81",   "metadata": {},   "source": [    "from sklearn.cluster import DBSCAN\n",    "\n",    "\n",    "# Apply DBSCAN on the standardized purchase behavior data\n",    "# Adjust eps and min_samples based on your data\n",    "dbscan = DBSCAN(eps=1.0, min_samples=5)\n",    "\n",    "# Fit DBSCAN model\n",    "df['DBSCAN_Cluster'] = dbscan.fit_predict(X_behavior)\n",    "\n",    "# Count clusters (-1 indicates noise points)\n",    "cluster_counts = df['DBSCAN_Cluster'].value_counts().sort_index()\n",    "print(\"DBSCAN Cluster Counts:\")\n",    "print(cluster_counts)\n",    "\n",    "# Calculate percentage of data points in each cluster\n",    "total_points = len(df)\n",    "cluster_percentages = (cluster_counts / total_points * 100).round(2)\n",    "print(\"\\nDBSCAN Cluster Percentages:\")\n",    "for cluster, percentage in cluster_percentages.items():\n",    "    status = \"Noise points\" if cluster == -1 else f\"Cluster {cluster}\"\n",    "    print(f\"{status}: {percentage}%\")\n",    "\n",    "# Apply PCA for visualization\n",    "pca = PCA(n_components=2)\n",    "X_pca = pca.fit_transform(X_behavior)\n",    "\n",    "# Visualize the DBSCAN clusters with PCA\n",    "plt.figure(figsize=(12, 8))\n",    "scatter = plt.scatter(X_pca[:, 0], X_pca[:, 1], c=df['DBSCAN_Cluster'],\n",    "                      cmap='viridis', s=60, alpha=0.7, edgecolors='black', linewidth=0.5)\n",    "plt.colorbar(scatter, label='DBSCAN Cluster')\n",    "plt.title('DBSCAN Clustering of Purchase Behavior (PCA View)', fontsize=16, pad=20)\n",    "plt.xlabel(f'First Principal Component ({pca.explained_variance_ratio_[0]:.1%} variance)')\n",    "plt.ylabel(f'Second Principal Component ({pca.explained_variance_ratio_[1]:.1%} variance)')\n",    "plt.grid(True, alpha=0.3)\n",    "plt.tight_layout()\n",    "plt.show()\n",    "\n",    "# Compare DBSCAN clusters with KMeans clusters\n",    "cross_tab = pd.crosstab(df['DBSCAN_Cluster'], df['PurchaseCluster'],\n",    "                        normalize='index') * 100\n",    "print(\"\\nCross-tabulation (%) of DBSCAN vs. KMeans clusters:\")\n",    "print(cross_tab.round(2))\n",    "\n",    "# Compare purchase patterns across DBSCAN clusters\n",    "purchase_metrics = purchase_behavior_features + ['Total_Purchases', 'Income', 'Total_Spending']\n",    "\n",    "# Get cluster summaries, handling -1 (noise) separately\n",    "cluster_summary = df.groupby('DBSCAN_Cluster')[purchase_metrics].mean().round(2)\n",    "print(\"\\nPurchase Behavior by DBSCAN Cluster:\")\n",    "print(cluster_summary)\n",    "\n",    "# Visualize key metrics by DBSCAN cluster\n",    "metrics_to_plot = ['NumWebPurchases', 'NumStorePurchases', 'NumCatalogPurchases', 'NumDealsPurchases']\n",    "\n",    "plt.figure(figsize=(14, 8))\n",    "ax = cluster_summary[metrics_to_plot].plot(kind='bar', figsize=(14, 8))\n",    "plt.title('Purchase Channel Usage by DBSCAN Cluster', fontsize=16, pad=20)\n",    "plt.xlabel('DBSCAN Cluster (-1 = Noise)')\n",    "plt.ylabel('Average Number of Purchases')\n",    "plt.legend(title='Purchase Channel')\n",    "plt.grid(axis='y', alpha=0.3)\n",    "\n",    "# Add value labels on bars\n",    "for container in ax.containers:\n",    "    ax.bar_label(container, fmt='%.1f', padding=3, fontsize=9)\n",    "\n",    "plt.tight_layout()\n",    "plt.show()\n",    "\n",    "# Characterize the clusters based on their features\n",    "print(\"\\nDBSCAN Cluster Characteristics:\")\n",    "for cluster in sorted(df['DBSCAN_Cluster'].unique()):\n",    "    if cluster == -1:\n",    "        print(\"\\nNoise Points:\")\n",    "    else:\n",    "        print(f\"\\nCluster {cluster}:\")\n",    "\n",    "    # Get top distinctive features\n",    "    cluster_data = cluster_summary.loc[cluster]\n",    "\n",    "    # For non-noise clusters, find distinguishing features\n",    "    if cluster != -1:\n",    "        other_clusters = cluster_summary.drop(cluster)\n"   ],   "outputs": [],   "execution_count": null  },  {   "cell_type": "code",   "id": "f379fbfbeefcb1de",   "metadata": {},   "source": [    "\n",    "\n",    "# Get average ratios per cluster\n",    "channel_ratios = df.groupby('PurchaseCluster')[['Web_Ratio', 'Store_Ratio', 'Catalog_Ratio', 'Deals_Ratio']].mean()\n",    "\n",    "# Create bar plot\n",    "ax = channel_ratios.plot(kind='bar', figsize=(10, 6), width=0.8)\n",    "plt.title('Preferred Purchase Channels by Cluster')\n",    "plt.xlabel('Cluster')\n",    "plt.ylabel('Proportion of Purchases')\n",    "plt.legend(title='Channel')\n",    "plt.xticks(rotation=0)\n",    "\n",    "# Add value labels on bars\n",    "for i in ax.containers:\n",    "    ax.bar_label(i, fmt='%.2f', padding=3)\n",    "\n",    "plt.tight_layout()\n",    "plt.show()\n",    "\n",    "# Print favorite channel for each cluster\n",    "for cluster in range(len(channel_ratios)):\n",    "    favorite_channel = channel_ratios.iloc[cluster].idxmax()\n",    "    ratio = channel_ratios.iloc[cluster][favorite_channel]\n",    "    print(f\"Cluster {cluster} favorite channel: {favorite_channel.replace('_Ratio', '')} ({ratio:.2%})\")\n"   ],   "outputs": [],   "execution_count": null  },  {   "cell_type": "code",   "id": "a11f941387242ebf",   "metadata": {},   "source": [    "\n",    "# 1. Compute your correlations\n",    "features = [\n",    "    'Income', 'Age', 'Total_Dependents', 'Tenure_Days',\n",    "    'Teenhome', 'Kidhome', 'Education',\n",    "    'Marital_Together', 'Marital_Single', 'Marital_Divorced', 'Marital_Widow',\n",    "    'PurchaseCluster'\n",    "]\n",    "corr = df[features].corr()\n",    "\n",    "# 2. Create a mask for the upper triangle\n",    "mask = np.triu(np.ones_like(corr, dtype=bool))\n",    "\n",    "# 3. Plot\n",    "plt.figure(figsize=(14, 12))\n",    "sns.heatmap(\n",    "    corr,\n",    "    mask=mask,\n",    "    annot=True,\n",    "    fmt='.2f',\n",    "    linewidths=.5,\n",    "    square=True,\n",    "    cbar_kws={'shrink': .8, 'label': 'Pearson ρ'},\n",    "    vmin=-1, vmax=1,\n",    "    center=0,\n",    "    cmap='vlag'   # a red‑to‑blue diverging map; you could switch back to 'coolwarm' if you prefer\n",    ")\n",    "\n",    "# 4. Styling\n",    "plt.title('Feature Correlation Matrix (Lower Triangle)', fontsize=18, pad=20)\n",    "plt.xticks(rotation=45, ha='right', fontsize=12)\n",    "plt.yticks(rotation=0, fontsize=12)\n",    "\n",    "# 5. Interpretation legend\n",    "plt.gcf().text(\n",    "    0.01, 0.01,\n",    "    \"■ Strong positive (> 0.7)\\n■ Strong negative (< -0.7)\\n■ Near zero: weak/no linear relation\",\n",    "    fontsize=10,\n",    "    bbox=dict(facecolor='white', alpha=0.8)\n",    ")\n",    "\n",    "plt.tight_layout()\n",    "plt.show()\n",    "\n",    "print(corr)\n"   ],   "outputs": [],   "execution_count": null  },  {   "metadata": {},   "cell_type": "code",   "source": [    "\n",    "\n",    "\n",    "\n",    "# -- Compute derived feature --\n",    "df['weighted_total_dependents'] = df['Kidhome'] * 2 + df['Teenhome']\n",    "\n",    "# -- Feature variants --\n",    "feature_sets = {\n",    "    'v1': ['Income', 'Age', 'Education', 'Total_Dependents'],\n",    "    'v2': ['Income', 'Age', 'Education', 'Teenhome', 'Kidhome'],\n",    "    'v3': ['Income', 'Age', 'Education',\n",    "           'Marital_Together', 'Marital_Single', 'Marital_Divorced', 'Marital_Widow',\n",    "           'Total_Dependents', 'Teenhome', 'Kidhome'],\n",    "    'v4': ['Income', 'Age', 'Education', 'Kidhome'],\n",    "    'v5': ['Income', 'Age', 'Education', 'weighted_total_dependents']\n",    "}\n",    "\n",    "y = df['PurchaseCluster']\n",    "\n",    "# -- Hyperparameter grid for RF --\n",    "param_grid_rf = {\n",    "    'n_estimators': [100, 300],\n",    "    'max_depth': [None, 10],\n",    "    'min_samples_split': [2, 5],\n",    "    'min_samples_leaf': [1, 2]\n",    "}\n",    "\n",    "# -- Cross-validation setup --\n",    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",    "\n",    "# -- Storage for summary results --\n",    "summary = []\n",    "\n",    "# -- Explore both Logistic Regression and RF Classifier for each variant --\n",    "for variant, feats in feature_sets.items():\n",    "    print(f\"\\n=== Variant: {variant} | Features: {feats} ===\")\n",    "    X = df[feats]\n",    "\n",    "    # Split\n",    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",    "\n",    "    # Scale\n",    "    scaler = StandardScaler()\n",    "    X_train_s = scaler.fit_transform(X_train)\n",    "    X_test_s = scaler.transform(X_test)\n",    "\n",    "    # -- Logistic Regression --\n",    "    lr = LogisticRegression(max_iter=1000)\n",    "    # CV Accuracy\n",    "    lr_cv_acc = cross_val_score(lr, scaler.fit_transform(X), y, scoring='accuracy', cv=kf)\n",    "    print(f\"LogisticRegression CV Accuracy: {lr_cv_acc.mean() * 100:.2f}% ± {lr_cv_acc.std() * 100:.2f}%\")\n",    "    # Test-set Accuracy\n",    "    lr.fit(X_train_s, y_train)\n",    "    lr_preds = lr.predict(X_test_s)\n",    "    lr_acc = accuracy_score(y_test, lr_preds) * 100\n",    "    print(f\"LogisticRegression Test Accuracy: {lr_acc:.2f}%\")\n",    "\n",    "    # -- Random Forest Classifier --\n",    "    rf = RandomForestClassifier(random_state=42)\n",    "    rf_cv_acc = cross_val_score(rf, scaler.fit_transform(X), y, scoring='accuracy', cv=kf)\n",    "    print(f\"RandomForest CV Accuracy: {rf_cv_acc.mean() * 100:.2f}% ± {rf_cv_acc.std() * 100:.2f}%\")\n",    "\n",    "    grid = GridSearchCV(rf, param_grid_rf, cv=5, scoring='accuracy', n_jobs=-1)\n",    "    grid.fit(X_train_s, y_train)\n",    "    best_rf = grid.best_estimator_\n",    "    rf_preds = best_rf.predict(X_test_s)\n",    "    rf_acc = accuracy_score(y_test, rf_preds) * 100\n",    "    print(f\"RandomForest Test Accuracy: {rf_acc:.2f}%\")\n",    "    print(f\"Best RF Params: {grid.best_params_}\")\n",    "\n",    "    # Save summary\n",    "    summary.append({\n",    "        'Variant': variant,\n",    "        'LR_CV_Acc': lr_cv_acc.mean() * 100,\n",    "        'LR_Test_Acc': lr_acc,\n",    "        'RF_CV_Acc': rf_cv_acc.mean() * 100,\n",    "        'RF_Test_Acc': rf_acc\n",    "    })\n",    "\n",    "# -- Summary DataFrame --\n",    "summary_df = pd.DataFrame(summary)\n",    "print(\"\\n=== Summary of All Variants (Percentage Accuracy) ===\")\n",    "print(summary_df)\n"   ],   "id": "e9bb566ee832f465",   "outputs": [],   "execution_count": null  },  {   "metadata": {},   "cell_type": "code",   "source": [    "from sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold, GridSearchCV\n",    "from sklearn.ensemble import RandomForestClassifier\n",    "from sklearn.linear_model import LogisticRegression\n",    "from sklearn.preprocessing import StandardScaler\n",    "from sklearn.metrics import accuracy_score, confusion_matrix, ConfusionMatrixDisplay\n",    "import matplotlib.pyplot as plt\n",    "import pandas as pd\n",    "import numpy as np\n",    "\n",    "# -- Log-transform skewed Income --\n",    "df['Income'] = np.log1p(df['Income'])\n",    "\n",    "# -- Weighted dependents feature --\n",    "df['weighted_total_dependents'] = df['Kidhome'] * 2 + df['Teenhome']\n",    "\n",    "# -- Feature sets (Education is ordinal) --\n",    "feature_sets = {\n",    "    'v1': ['Income', 'Age', 'Education', 'Total_Dependents'],\n",    "    'v2': ['Income', 'Age', 'Education', 'Teenhome', 'Kidhome'],\n",    "    'v3': ['Income', 'Age', 'Education',\n",    "           'Marital_Together', 'Marital_Single', 'Marital_Divorced', 'Marital_Widow',\n",    "           'Total_Dependents', 'Teenhome', 'Kidhome'],\n",    "    'v4': ['Income', 'Age', 'Education', 'Kidhome'],\n",    "    'v5': ['Income', 'Age', 'Education', 'weighted_total_dependents']\n",    "}\n",    "\n",    "y = df['PurchaseCluster']\n",    "\n",    "# -- Hyperparameter grids --\n",    "param_grid_rf = {\n",    "    'n_estimators': [100, 300],\n",    "    'max_depth': [None, 10],\n",    "    'min_samples_split': [2, 5],\n",    "    'min_samples_leaf': [1, 2]\n",    "}\n",    "\n",    "param_grid_lr = {\n",    "    'C': [0.01, 0.1, 1, 10],\n",    "    'penalty': ['l2'],\n",    "    'solver': ['lbfgs', 'liblinear']\n",    "}\n",    "\n",    "# -- Stratified Cross-validation --\n",    "kf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",    "\n",    "summary = []\n",    "\n",    "for variant, feats in feature_sets.items():\n",    "    print(f\"\\n=== Variant: {variant} | Features: {feats} ===\")\n",    "    X = df[feats]\n",    "\n",    "    X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, test_size=0.2, random_state=42)\n",    "\n",    "    # Scaling\n",    "    scaler = StandardScaler()\n",    "    X_train_s = scaler.fit_transform(X_train)\n",    "    X_test_s = scaler.transform(X_test)\n",    "\n",    "    # -- Logistic Regression (tuned) --\n",    "    grid_lr = GridSearchCV(LogisticRegression(max_iter=1000), param_grid_lr, cv=kf, scoring='accuracy', n_jobs=-1)\n",    "    grid_lr.fit(X_train_s, y_train)\n",    "    best_lr = grid_lr.best_estimator_\n",    "    lr_preds = best_lr.predict(X_test_s)\n",    "    lr_acc = accuracy_score(y_test, lr_preds) * 100\n",    "    lr_cv_acc = cross_val_score(best_lr, scaler.fit_transform(X), y, scoring='accuracy', cv=kf)\n",    "    print(f\"LogisticRegression CV Accuracy: {lr_cv_acc.mean()*100:.2f}% ± {lr_cv_acc.std()*100:.2f}%\")\n",    "    print(f\"LogisticRegression Test Accuracy: {lr_acc:.2f}%\")\n",    "    print(\"Best LR Params:\", grid_lr.best_params_)\n",    "\n",    "    # Confusion matrix for LR\n",    "    print(\"confusion_matrix for lr:\")\n",    "\n",    "    print(confusion_matrix(y_test, lr_preds))\n",    "\n",    "    # -- Random Forest Classifier (tuned) --\n",    "    grid_rf = GridSearchCV(RandomForestClassifier(random_state=42), param_grid_rf, cv=kf, scoring='accuracy', n_jobs=-1)\n",    "    grid_rf.fit(X_train_s, y_train)\n",    "    best_rf = grid_rf.best_estimator_\n",    "    rf_preds = best_rf.predict(X_test_s)\n",    "    rf_acc = accuracy_score(y_test, rf_preds) * 100\n",    "    rf_cv_acc = cross_val_score(best_rf, scaler.fit_transform(X), y, scoring='accuracy', cv=kf)\n",    "    print(f\"RandomForest CV Accuracy: {rf_cv_acc.mean()*100:.2f}% ± {rf_cv_acc.std()*100:.2f}%\")\n",    "    print(f\"RandomForest Test Accuracy: {rf_acc:.2f}%\")\n",    "    print(\"Best RF Params:\", grid_rf.best_params_)\n",    "\n",    "\n",    "    print(\"confusion_matrix for rf:\")\n",    "    print(confusion_matrix(y_test, rf_preds))\n",    "\n",    "    # -- Store results --\n",    "    summary.append({\n",    "        'Variant': variant,\n",    "        'LR_CV_Acc': lr_cv_acc.mean() * 100,\n",    "        'LR_Test_Acc': lr_acc,\n",    "        'RF_CV_Acc': rf_cv_acc.mean() * 100,\n",    "        'RF_Test_Acc': rf_acc\n",    "    })\n",    "\n",    "# -- Summary Results --\n",    "summary_df = pd.DataFrame(summary)\n",    "print(\"\\n=== Summary of All Variants (Final) ===\")\n",    "print(summary_df)\n"   ],   "id": "9cdecfd384f0a793",   "outputs": [],   "execution_count": null  },  {   "metadata": {},   "cell_type": "code",   "source": [    "\n",    "# -- Log-transform skewed Income --\n",    "df['Income'] = np.log1p(df['Income'])\n",    "\n",    "# -- Weighted dependents feature --\n",    "df['weighted_total_dependents'] = df['Kidhome'] * 2 + df['Teenhome']\n",    "\n",    "# -- v3 Feature set --\n",    "features_v3 = [\n",    "    'Income', 'Age', 'Education',\n",    "    'Marital_Together', 'Marital_Single', 'Marital_Divorced', 'Marital_Widow',\n",    "    'Total_Dependents', 'Teenhome', 'Kidhome'\n",    "]\n",    "X = df[features_v3]\n",    "y = df['PurchaseCluster']\n",    "\n",    "# -- Hyperparameter grid for Random Forest --\n",    "param_grid_rf = {\n",    "    'n_estimators': [100, 300],\n",    "    'max_depth': [None, 10],\n",    "    'min_samples_split': [2, 5],\n",    "    'min_samples_leaf': [1, 2]\n",    "}\n",    "\n",    "# -- Stratified Cross-validation --\n",    "kf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",    "\n",    "# -- Train/Test Split --\n",    "X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, test_size=0.2, random_state=42)\n",    "\n",    "# -- Scaling --\n",    "scaler = StandardScaler()\n",    "X_train_s = scaler.fit_transform(X_train)\n",    "X_test_s = scaler.transform(X_test)\n",    "\n",    "# -- Random Forest Classifier (tuned) --\n",    "grid_rf = GridSearchCV(RandomForestClassifier(random_state=42), param_grid_rf, cv=kf, scoring='accuracy', n_jobs=-1)\n",    "grid_rf.fit(X_train_s, y_train)\n",    "best_rf = grid_rf.best_estimator_\n",    "rf_preds = best_rf.predict(X_test_s)\n",    "rf_acc = accuracy_score(y_test, rf_preds) * 100\n",    "rf_cv_acc = cross_val_score(best_rf, scaler.fit_transform(X), y, scoring='accuracy', cv=kf)\n",    "\n",    "print(f\"RandomForest CV Accuracy: {rf_cv_acc.mean()*100:.2f}% ± {rf_cv_acc.std()*100:.2f}%\")\n",    "print(f\"RandomForest Test Accuracy: {rf_acc:.2f}%\")\n",    "print(\"Best RF Params:\", grid_rf.best_params_)\n",    "\n",    "print(\"confusion_matrix for rf:\")\n",    "print(confusion_matrix(y_test, rf_preds))\n",    "\n",    "# -- Summary Results --\n",    "summary = [{\n",    "    'Variant': 'v3',\n",    "    'RF_CV_Acc': rf_cv_acc.mean() * 100,\n",    "    'RF_Test_Acc': rf_acc\n",    "}]\n",    "summary_df = pd.DataFrame(summary)\n",    "print(\"\\n=== Summary for v3 Random Forest ===\")\n",    "print(summary_df)\n",    "\n"   ],   "id": "cc9856a2f22a4dfa",   "outputs": [],   "execution_count": null  } ], "metadata": {  "kernelspec": {   "display_name": "Python 3 (ipykernel)",   "language": "python",   "name": "python3"  },  "language_info": {   "codemirror_mode": {    "name": "ipython",    "version": 3   },   "file_extension": ".py",   "mimetype": "text/x-python",   "name": "python",   "nbconvert_exporter": "python",   "pygments_lexer": "ipython3",   "version": "3.12.2"  } }, "nbformat": 4, "nbformat_minor": 5}