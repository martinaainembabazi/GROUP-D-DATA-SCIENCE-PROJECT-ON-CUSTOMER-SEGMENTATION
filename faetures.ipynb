{
 "cells": [
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "64a25b3ad9cfc8f4"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from enum import unique\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "# import matplotlib as mpl\n",
    "import seaborn as sns\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# from sklearn.preprocessing import LabelEncoder,StandardScaler\n",
    "# from  sklearn.feature_extraction.text import CountVectorizer\n",
    "# from sklearn import metrics\n",
    "# from xgboost import XGBRegressor\n",
    "# from datetime import datetime\n",
    "\n",
    "\n",
    "\n",
    "import warnings\n",
    "\n",
    "from fontTools.subset import subset\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ],
   "id": "28232ce299f37a2"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "df = pd.read_csv('Customer_Segmentation.csv')\n",
    "df.head()\n"
   ],
   "id": "6af440a3945fef2c"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Finding missing values",
   "id": "27391907b4408c92"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "counts_missing = df.isnull().sum()\n",
    "missing_percentage = (counts_missing / len(df)) * 100\n",
    "\n",
    "#summary of missing values\n",
    "missing_summary = pd.DataFrame({\n",
    "    'Missing Values': counts_missing,\n",
    "    'Percentage': missing_percentage\n",
    "}).reset_index().rename(columns={'index': 'Column'})\n",
    "print(missing_summary)"
   ],
   "id": "eb73f3d287a60895"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Filling missing data\n",
    "## Income\n",
    "we have 24 missing values in the Income column, which is significant. We will handle this by using median imputation.\n",
    "\n",
    "1. Median is robust to outliers, which is important for income data that may have extreme values.\n",
    "2. It preserves the distribution of the data better than mean imputation."
   ],
   "id": "d15109eb00bcd6df"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "#we decided to use median impoutation for the missing values in Income column.This is because of the following reasons\n",
    "\n",
    "df['Income'].fillna(df['Income'].median(), inplace=True)\n",
    "print(df[\"Income\"].isnull().sum())\n"
   ],
   "id": "d72c628b5f72a83f"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Finding duplicates\n",
    "No duplicates found"
   ],
   "id": "30fcbf89e26a17e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "df.duplicated().sum()",
   "id": "85088bc76f8e82d0"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Date Formatting\n",
    "Dt_Customer is in the format of 'dd-mm-yyyy', we will convert it to a datetime object for easier analysis."
   ],
   "id": "11c49b5749078d67"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "df['Dt_Customer'] = pd.to_datetime(df['Dt_Customer'], format='%d-%m-%Y', errors='coerce')\n",
    "print(\"First 20 enrollment dates:\")\n",
    "print(df['Dt_Customer'].head(20))\n",
    "\n",
    "print('\\nNull vales after formatting: ')\n",
    "print(df[df['Dt_Customer'].isnull()])"
   ],
   "id": "771cfe3f270117c3"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Checking for constant columns\n",
    "Constant columns are those that have the same value for all rows. These columns do not provide any useful information for analysis or modeling, so they can be safely removed."
   ],
   "id": "b1ffd04babcd74b2"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "\n",
    "constant_columns = [col for col in df.columns if df[col].nunique() == 1]\n",
    "print(constant_columns)\n"
   ],
   "id": "b1b67cde48cf812d"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Dropping constant\n",
    "\n",
    "### found 2\n",
    "- AcceptedCmp3\n",
    "- Z_Revenue\n",
    "\n",
    "We found 2 constant columns: 'AcceptedCmp3' and 'Z_Revenue'. These columns have the same value for all rows, which means they do not provide any useful information for analysis or modeling. Therefore, we will drop them from the dataset."
   ],
   "id": "6e5724941e179c45"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "\n",
    "# Remove constant columns\n",
    "df = df.drop(columns=constant_columns)\n",
    "\n",
    "print(f\"Removed {len(constant_columns)} constant columns: {constant_columns}\")"
   ],
   "id": "dde15cc53082a954"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Finding Outliers\n",
    "Using the Interquartile Range (IQR) method to identify outliers in numerical columns. The IQR is a measure of statistical dispersion and is used to detect outliers by calculating the range between the first quartile (Q1) and the third quartile (Q3).\n"
   ],
   "id": "afed83ca2d3e4132"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "#identifying outliers using IQR method\n",
    "numerical_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "\n",
    "# Exclude ID and boolean columns\n",
    "bool_cols = [col for col in df.columns if np.all(np.isin(df[col].unique(), [0, 1]))]\n",
    "numerical_cols = [col for col in numerical_cols if col not in bool_cols and col != 'ID']\n",
    "# list of Outlier\n",
    "def outlier_summary(dataframe):\n",
    "\n",
    "    outlier_info = []\n",
    "    total_outliers = 0\n",
    "\n",
    "    for col in numerical_cols:\n",
    "        # Calculate IQR\n",
    "        Q1 = dataframe[col].quantile(0.25)\n",
    "        Q3 = dataframe[col].quantile(0.75)\n",
    "        IQR = Q3 - Q1\n",
    "\n",
    "        # Find outliers\n",
    "        lower_bound = Q1 - 1.5 * IQR\n",
    "        upper_bound = Q3 + 1.5 * IQR\n",
    "        outliers = dataframe[(dataframe[col] < lower_bound) | (dataframe[col] > upper_bound)]\n",
    "        outlier_count = len(outliers)\n",
    "\n",
    "        if outlier_count > 0:\n",
    "            outlier_percentage = (outlier_count / len(dataframe)) * 100\n",
    "            outlier_info.append((col, outlier_count, outlier_percentage))\n",
    "            total_outliers += outlier_count\n",
    "\n",
    "    print(f\"Columns with outliers: {len(outlier_info)}\")\n",
    "    print(f\"Total outliers: {total_outliers}\")\n",
    "\n",
    "    if outlier_info:\n",
    "        print(\"Columns with outliers:\")\n",
    "        print()\n",
    "        # Sort by outlier count (descending)\n",
    "        #outlier_info.sort(key=lambda x: x[1], reverse=True)\n",
    "\n",
    "        for col, count, percentage in outlier_info:\n",
    "            print(f\"{col}: {count} outliers ({percentage:.1f}%)\")\n",
    "    else:\n",
    "        print(\"No outliers detected in any column!\")\n",
    "\n",
    "# Run the outlier summary\n",
    "outlier_summary(df)\n"
   ],
   "id": "2d15662bd20d0399"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "#visualize the outliers using boxplots\n",
    "def plot_outliers(dataframe):\n",
    "    n_cols = 3\n",
    "    n_plots = len(numerical_cols)\n",
    "    n_rows = (n_plots + n_cols - 1) // n_cols  # Ceiling division\n",
    "\n",
    "    plt.figure(figsize=(5 * n_cols, 4 * n_rows))\n",
    "\n",
    "    for i, col in enumerate(numerical_cols):\n",
    "        plt.subplot(n_rows, n_cols, i + 1)\n",
    "        sns.boxplot(x=dataframe[col])\n",
    "        plt.title(f'Boxplot of {col}')\n",
    "        plt.xlabel(col)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# Run the outlier visualization\n",
    "plot_outliers(df)\n"
   ],
   "id": "f0817b91ceea3de9"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## handling the outliers\n",
    "# We will use the capping method to handle outliers. This involves replacing outliers with the nearest non-outlier value, which is determined by the IQR method.\n"
   ],
   "id": "735bf449274f06d2"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Capping method to handle outliers\n",
    "def cap_outliers(dataframe):\n",
    "\n",
    "    df_capped = dataframe.copy()\n",
    "\n",
    "    for col in numerical_cols:\n",
    "        # Calculate IQR\n",
    "        Q1 = df_capped[col].quantile(0.25)\n",
    "        Q3 = df_capped[col].quantile(0.75)\n",
    "        IQR = Q3 - Q1\n",
    "        lower_bound = Q1 - 1.5 * IQR\n",
    "        upper_bound = Q3 + 1.5 * IQR\n",
    "\n",
    "        # Count outliers before capping\n",
    "        outliers_before = len(df_capped[(df_capped[col] < lower_bound) | (df_capped[col] > upper_bound)])\n",
    "\n",
    "        # Apply capping\n",
    "        df_capped[col] = df_capped[col].clip(lower=lower_bound, upper=upper_bound)\n",
    "\n",
    "        # Count outliers after capping\n",
    "        outliers_after = len(df_capped[(df_capped[col] < lower_bound) | (df_capped[col] > upper_bound)])\n",
    "\n",
    "        if outliers_before > 0:\n",
    "            print(f\"{col}: {outliers_before} outliers capped\")\n",
    "\n",
    "    print(f\"\\nOutlier capping completed!\")\n",
    "    return df_capped #return the capped DataFrame while preserving the original DataFrame\n",
    "\n",
    "# Apply capping to the dataset\n",
    "df_capped = cap_outliers(df)\n",
    "\n",
    "\n",
    "\n"
   ],
   "id": "a3c275de0c313405"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Visualizatin after capping\n",
    "# Visualize the capped data using boxplots\n",
    "We will create boxplots for each numerical column to visualize the effect of capping on outliers. The boxplots will show the distribution of the data and highlight any remaining outliers after capping."
   ],
   "id": "7667922eec21ad91"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "\n",
    "\n",
    "# Also created a simple boxplot of just the capped data\n",
    "def plot_capped_data_only(capped_df):\n",
    "\n",
    "    if 'ID' in numerical_cols:\n",
    "        numerical_cols.remove('ID')\n",
    "\n",
    "    n_cols = 3\n",
    "    n_plots = len(numerical_cols)\n",
    "    n_rows = (n_plots + n_cols - 1) // n_cols\n",
    "\n",
    "    plt.figure(figsize=(5 * n_cols, 4 * n_rows))\n",
    "\n",
    "    for i, col in enumerate(numerical_cols):\n",
    "        plt.subplot(n_rows, n_cols, i + 1)\n",
    "        sns.boxplot(x=capped_df[col], color='lightgreen')\n",
    "        plt.title(f'Capped Data: {col}')\n",
    "        plt.xlabel(col)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "print(\"Boxplots of capped data only:\")\n",
    "plot_capped_data_only(df_capped)\n",
    "\n",
    "# Check if the cleaned customer CSV exists\n",
    "if os.path.exists('cleaned_customer_segmentation.csv'):\n",
    "    print(f\"Existing cleaned_customer_segmentation.csv found, removing it...\")\n",
    "    os.remove('cleaned_customer_segmentation.csv')\n",
    "    print(\"File removed successfully.\")\n",
    "\n",
    "# Save the new cleaned dataset\n",
    "df_capped.to_csv('cleaned_customer_segmentation.csv', index=False)\n",
    "print(f\"New cleaned_customer_segmentation.csv saved successfully ({os.path.getsize('cleaned_customer_segmentation.csv')/1024:.2f} KB)\")\n",
    "\n",
    "# Verify the file was saved correctly\n",
    "if os.path.exists('cleaned_customer_segmentation.csv'):\n",
    "    # Load the cleaned dataset to confirm it was saved properly\n",
    "    df_cleaned = pd.read_csv('cleaned_customer_segmentation.csv')\n",
    "    print(f\"Loaded cleaned dataset with shape: {df_cleaned.shape}\")"
   ],
   "id": "fde85eb79cf8ffb8"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Feature Egineering\n",
    "Feature engineering is the process of using domain knowledge to extract features from raw data that make machine learning algorithms work. In this section, we will create new features based on the existing data in the dataset."
   ],
   "id": "798242f0ff5a6728"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "# Load data\n",
    "df = pd.read_csv('cleaned_customer_segmentation.csv')\n"
   ],
   "id": "8ec98410d33c528c"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Text encoding\n",
    "### Education\n",
    "- we chose ordinal encoding to put into perspective the increasing level of education\n",
    "1. first we will find the unique eduaction levels"
   ],
   "id": "11aa11abc93e206c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "\n",
    "\n",
    "unique_eduaction_levels= df['Education'].unique()\n",
    "\n",
    "print(unique_eduaction_levels)"
   ],
   "id": "bed2eb288c8cf688"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## next steps\n",
    "2. supply ordinal encoder with oredered categories\n",
    "\n",
    "`['Basic', '2n Cycle', 'Graduation', 'Master', 'PhD']\n",
    "`\n",
    "\n",
    "3. fit the education column"
   ],
   "id": "45e143f4c4a12fe"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "\n",
    "ordered_education_categories = ['Basic', '2n Cycle', 'Graduation', 'Master', 'PhD']\n",
    "ordinal_encoder = OrdinalEncoder(categories=[ordered_education_categories])\n",
    "\n",
    "education_encoded = ordinal_encoder.fit_transform(df[['Education']])\n",
    "\n",
    "df['Education'] = education_encoded"
   ],
   "id": "5f0b45857ae06112"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "\n",
    "print('Unique Marital Status:')\n",
    "print(df['Marital_Status'].unique())\n",
    "print('\\nMarital Status value counts:')\n",
    "print(df['Marital_Status'].value_counts())"
   ],
   "id": "89c5dbd3dcd149e1"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Marital status evaluation\n",
    "\n",
    "### Unique Marital Status:\n",
    "`['Single' 'Together' 'Married' 'Divorced' 'Widow' 'Alone' 'Absurd' 'YOLO']\n",
    "`\n",
    "### Marital Status value counts:\n",
    "`Marital_Status,\n",
    "Married     864,\n",
    "Together    580,\n",
    "Single      480,\n",
    "Divorced    232,\n",
    "Widow        77,\n",
    "Alone         3,\n",
    "Absurd        2`\n",
    "\n",
    "## Deduction\n",
    "### Wrong data\n",
    "- Yolo alone and absurd are wrong data entries.\n",
    "- Alone is the same as single and its values can be replaced with single\n",
    "- Absurd and yolo will be replaced with the mode\n",
    "\n"
   ],
   "id": "2244b860c8ae5f43"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# replace Alone with Single\n",
    "df['Marital_Status'] = df['Marital_Status'].replace('Alone', 'Single')\n",
    "print('\\nMode: ',df['Marital_Status'].mode()[0])\n",
    "# Absurd and yolo  replaced with the mode\n",
    "df['Marital_Status'] = df['Marital_Status'].replace(['YOLO','Absurd'], df['Marital_Status'].mode()[0])\n",
    "\n",
    "#now check for the current state\n",
    "print('Unique Marital Status:')\n",
    "print(df['Marital_Status'].unique())\n",
    "print('\\nMarital Status value counts:')\n",
    "print(df['Marital_Status'].value_counts())\n"
   ],
   "id": "79263df768fd0b16"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### marital status encoding\n",
    "We went with **one hot encoding**,\n",
    "this is because ~~nominal encoding~~ would create a non existent _hierachy_"
   ],
   "id": "1de09e90d6c56034"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# One-hot encode marital status\n",
    "marital_dummies = pd.get_dummies(df['Marital_Status'], prefix='Marital')\n",
    "df = pd.concat([df.drop('Marital_Status', axis=1), marital_dummies], axis=1)\n",
    "\n",
    "print(df.info())\n"
   ],
   "id": "3d35645ec74da013"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "\n",
    "# Feature engineering with corrected date parsing\n",
    "try:\n",
    "\n",
    "    df['Dt_Customer'] = pd.to_datetime(df['Dt_Customer'], format='mixed', dayfirst=True)\n",
    "except ValueError:\n",
    "\n",
    "    df['Dt_Customer'] = pd.to_datetime(df['Dt_Customer'], format='%d-%m-%Y', errors='coerce')\n",
    "\n",
    "\n",
    "df['Age'] = 2025- df['Year_Birth']\n",
    "df['Total_Dependents'] = df['Kidhome'] + df['Teenhome']\n",
    "df['Is_Parent'] = (df['Total_Dependents'] > 0).astype(int)\n",
    "df['Total_Spending'] = df[['MntWines','MntFruits','MntMeatProducts','MntFishProducts','MntSweetProducts','MntGoldProds']].sum(axis=1)\n",
    "df['Total_Purchases'] = df[['NumWebPurchases','NumCatalogPurchases','NumStorePurchases']].sum(axis=1)\n",
    "df['Average_Spent'] = df['Total_Spending']/df['Total_Purchases'].replace(0,1)\n",
    "df['Tenure_Days'] = (pd.to_datetime('today') - df['Dt_Customer']).dt.days\n",
    "df['Total_Accepted_Cmp']=df[['AcceptedCmp1','AcceptedCmp2','AcceptedCmp3','AcceptedCmp4','AcceptedCmp5']].sum(axis=1)\n",
    "df['Web_Ratio'] = df['NumWebPurchases'] / df['Total_Purchases']\n",
    "df['Store_Ratio'] = df['NumStorePurchases'] / df['Total_Purchases']\n",
    "df['Catalog_Ratio'] = df['NumCatalogPurchases'] / df['Total_Purchases']\n",
    "df['Deals_Ratio'] = df['NumDealsPurchases'] / df['Total_Purchases']\n",
    "df['Ever_Accepted_Campaign'] = (df['Total_Accepted_Cmp'] > 0).astype(int)\n",
    "\n",
    "# Verify date conversion\n",
    "print(\"\\nDate conversion samples:\")\n",
    "print(df[['Dt_Customer', 'Tenure_Days']].head())\n",
    "\n",
    "# Drop unnecessary columns\n",
    "# df = df.drop(['ID', 'Year_Birth', 'Z_CostContact', 'Z_Revenue','Complain','Response','Kidhome', 'Teenhome','AcceptedCmp1','MntWines','MntFruits','MntMeatProducts','MntFishProducts','MntSweetProducts','MntGoldProds','AcceptedCmp2','AcceptedCmp3','AcceptedCmp4','AcceptedCmp5','MntWines','MntFruits','MntMeatProducts','MntFishProducts','MntSweetProducts','MntGoldProds'], axis=1, errors='ignore')\n",
    "\n",
    "# Check if the featured customer CSV exists\n",
    "if os.path.exists('featured_customer_segmentation.csv'):\n",
    "    print(f\"Existing featured_customer_segmentation.csv found, removing it...\")\n",
    "    os.remove('featured_customer_segmentation.csv')\n",
    "    print(\"File removed successfully.\")\n",
    "\n",
    "# Save the new featured dataset\n",
    "df.to_csv('featured_customer_segmentation.csv', index=False)\n",
    "print(f\"New featured_customer_segmentation.csv saved successfully ({os.path.getsize('featured_customer_segmentation.csv')/1024:.2f} KB)\")\n",
    "\n"
   ],
   "id": "fb564c002d7128fc"
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
