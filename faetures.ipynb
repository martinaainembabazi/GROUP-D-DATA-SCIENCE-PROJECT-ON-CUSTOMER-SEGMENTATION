{
 "cells": [
  {
   "cell_type": "code",
   "id": "64a25b3ad9cfc8f4",
   "metadata": {},
   "source": [],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "28232ce299f37a2",
   "metadata": {},
   "source": [
    "from enum import unique\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "# import matplotlib as mpl\n",
    "import seaborn as sns\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# from sklearn.preprocessing import LabelEncoder,StandardScaler\n",
    "# from  sklearn.feature_extraction.text import CountVectorizer\n",
    "# from sklearn import metrics\n",
    "# from xgboost import XGBRegressor\n",
    "# from datetime import datetime\n",
    "\n",
    "\n",
    "\n",
    "import warnings\n",
    "\n",
    "from fontTools.subset import subset\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "6af440a3945fef2c",
   "metadata": {},
   "source": [
    "df = pd.read_csv('Customer_Segmentation.csv')\n",
    "df.head()\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "27391907b4408c92",
   "metadata": {},
   "source": [
    "## Finding missing values"
   ]
  },
  {
   "cell_type": "code",
   "id": "eb73f3d287a60895",
   "metadata": {},
   "source": [
    "counts_missing = df.isnull().sum()\n",
    "missing_percentage = (counts_missing / len(df)) * 100\n",
    "\n",
    "#summary of missing values\n",
    "missing_summary = pd.DataFrame({\n",
    "    'Missing Values': counts_missing,\n",
    "    'Percentage': missing_percentage\n",
    "}).reset_index().rename(columns={'index': 'Column'})\n",
    "print(missing_summary)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "d15109eb00bcd6df",
   "metadata": {},
   "source": [
    "# Filling missing data\n",
    "## Income\n",
    "we have 24 missing values in the Income column, which is significant. We will handle this by using median imputation.\n",
    "\n",
    "1. Median is robust to outliers, which is important for income data that may have extreme values.\n",
    "2. It preserves the distribution of the data better than mean imputation."
   ]
  },
  {
   "cell_type": "code",
   "id": "d72c628b5f72a83f",
   "metadata": {},
   "source": [
    "#we decided to use median impoutation for the missing values in Income column.This is because of the following reasons\n",
    "\n",
    "df['Income'].fillna(df['Income'].median(), inplace=True)\n",
    "print(df[\"Income\"].isnull().sum())\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "30fcbf89e26a17e",
   "metadata": {},
   "source": [
    "## Finding duplicates\n",
    "No duplicates found"
   ]
  },
  {
   "cell_type": "code",
   "id": "85088bc76f8e82d0",
   "metadata": {},
   "source": [
    "df.duplicated().sum()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "11c49b5749078d67",
   "metadata": {},
   "source": [
    "## Date Formatting\n",
    "Dt_Customer is in the format of 'dd-mm-yyyy', we will convert it to a datetime object for easier analysis."
   ]
  },
  {
   "cell_type": "code",
   "id": "771cfe3f270117c3",
   "metadata": {},
   "source": [
    "df['Dt_Customer'] = pd.to_datetime(df['Dt_Customer'], format='%d-%m-%Y', errors='coerce')\n",
    "print(\"First 20 enrollment dates:\")\n",
    "print(df['Dt_Customer'].head(20))\n",
    "\n",
    "print('\\nNull vales after formatting: ')\n",
    "print(df[df['Dt_Customer'].isnull()])"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "b1ffd04babcd74b2",
   "metadata": {},
   "source": [
    "## Checking for constant columns\n",
    "Constant columns are those that have the same value for all rows. These columns do not provide any useful information for analysis or modeling, so they can be safely removed."
   ]
  },
  {
   "cell_type": "code",
   "id": "b1b67cde48cf812d",
   "metadata": {},
   "source": [
    "\n",
    "constant_columns = [col for col in df.columns if df[col].nunique() == 1]\n",
    "print(constant_columns)\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "6e5724941e179c45",
   "metadata": {},
   "source": [
    "## Dropping constant\n",
    "\n",
    "### found 2\n",
    "- AcceptedCmp3\n",
    "- Z_Revenue\n",
    "\n",
    "We found 2 constant columns: 'AcceptedCmp3' and 'Z_Revenue'. These columns have the same value for all rows, which means they do not provide any useful information for analysis or modeling. Therefore, we will drop them from the dataset."
   ]
  },
  {
   "cell_type": "code",
   "id": "dde15cc53082a954",
   "metadata": {},
   "source": [
    "\n",
    "# Remove constant columns\n",
    "df = df.drop(columns=constant_columns)\n",
    "\n",
    "print(f\"Removed {len(constant_columns)} constant columns: {constant_columns}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "afed83ca2d3e4132",
   "metadata": {},
   "source": [
    "## Finding Outliers\n",
    "Using the Interquartile Range (IQR) method to identify outliers in numerical columns. The IQR is a measure of statistical dispersion and is used to detect outliers by calculating the range between the first quartile (Q1) and the third quartile (Q3).\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "2d15662bd20d0399",
   "metadata": {},
   "source": [
    "#identifying outliers using IQR method\n",
    "numerical_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "\n",
    "# Exclude ID and boolean columns\n",
    "bool_cols = [col for col in df.columns if np.all(np.isin(df[col].unique(), [0, 1]))]\n",
    "numerical_cols = [col for col in numerical_cols if col not in bool_cols and col != 'ID']\n",
    "# list of Outlier\n",
    "def outlier_summary(dataframe):\n",
    "\n",
    "    outlier_info = []\n",
    "    total_outliers = 0\n",
    "\n",
    "    for col in numerical_cols:\n",
    "        # Calculate IQR\n",
    "        Q1 = dataframe[col].quantile(0.25)\n",
    "        Q3 = dataframe[col].quantile(0.75)\n",
    "        IQR = Q3 - Q1\n",
    "\n",
    "        # Find outliers\n",
    "        lower_bound = Q1 - 1.5 * IQR\n",
    "        upper_bound = Q3 + 1.5 * IQR\n",
    "        outliers = dataframe[(dataframe[col] < lower_bound) | (dataframe[col] > upper_bound)]\n",
    "        outlier_count = len(outliers)\n",
    "\n",
    "        if outlier_count > 0:\n",
    "            outlier_percentage = (outlier_count / len(dataframe)) * 100\n",
    "            outlier_info.append((col, outlier_count, outlier_percentage))\n",
    "            total_outliers += outlier_count\n",
    "\n",
    "    print(f\"Columns with outliers: {len(outlier_info)}\")\n",
    "    print(f\"Total outliers: {total_outliers}\")\n",
    "\n",
    "    if outlier_info:\n",
    "        print(\"Columns with outliers:\")\n",
    "        print()\n",
    "        # Sort by outlier count (descending)\n",
    "        #outlier_info.sort(key=lambda x: x[1], reverse=True)\n",
    "\n",
    "        for col, count, percentage in outlier_info:\n",
    "            print(f\"{col}: {count} outliers ({percentage:.1f}%)\")\n",
    "    else:\n",
    "        print(\"No outliers detected in any column!\")\n",
    "\n",
    "# Run the outlier summary\n",
    "outlier_summary(df)\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "f0817b91ceea3de9",
   "metadata": {},
   "source": [
    "#visualize the outliers using boxplots\n",
    "def plot_outliers(dataframe):\n",
    "    n_cols = 3\n",
    "    n_plots = len(numerical_cols)\n",
    "    n_rows = (n_plots + n_cols - 1) // n_cols  # Ceiling division\n",
    "\n",
    "    plt.figure(figsize=(5 * n_cols, 4 * n_rows))\n",
    "\n",
    "    for i, col in enumerate(numerical_cols):\n",
    "        plt.subplot(n_rows, n_cols, i + 1)\n",
    "        sns.boxplot(x=dataframe[col])\n",
    "        plt.title(f'Boxplot of {col}')\n",
    "        plt.xlabel(col)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# Run the outlier visualization\n",
    "plot_outliers(df)\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "735bf449274f06d2",
   "metadata": {},
   "source": [
    "## handling the outliers\n",
    "# We will use the capping method to handle outliers. This involves replacing outliers with the nearest non-outlier value, which is determined by the IQR method.\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "a3c275de0c313405",
   "metadata": {},
   "source": [
    "# Capping method to handle outliers\n",
    "def cap_outliers(dataframe):\n",
    "\n",
    "    df_capped = dataframe.copy()\n",
    "\n",
    "    for col in numerical_cols:\n",
    "        # Calculate IQR\n",
    "        Q1 = df_capped[col].quantile(0.25)\n",
    "        Q3 = df_capped[col].quantile(0.75)\n",
    "        IQR = Q3 - Q1\n",
    "        lower_bound = Q1 - 1.5 * IQR\n",
    "        upper_bound = Q3 + 1.5 * IQR\n",
    "\n",
    "        # Count outliers before capping\n",
    "        outliers_before = len(df_capped[(df_capped[col] < lower_bound) | (df_capped[col] > upper_bound)])\n",
    "\n",
    "        # Apply capping\n",
    "        df_capped[col] = df_capped[col].clip(lower=lower_bound, upper=upper_bound)\n",
    "\n",
    "        # Count outliers after capping\n",
    "        outliers_after = len(df_capped[(df_capped[col] < lower_bound) | (df_capped[col] > upper_bound)])\n",
    "\n",
    "        if outliers_before > 0:\n",
    "            print(f\"{col}: {outliers_before} outliers capped\")\n",
    "\n",
    "    print(f\"\\nOutlier capping completed!\")\n",
    "    return df_capped #return the capped DataFrame while preserving the original DataFrame\n",
    "\n",
    "# Apply capping to the dataset\n",
    "df_capped = cap_outliers(df)\n",
    "\n",
    "\n",
    "\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "7667922eec21ad91",
   "metadata": {},
   "source": [
    "## Visualizatin after capping\n",
    "# Visualize the capped data using boxplots\n",
    "We will create boxplots for each numerical column to visualize the effect of capping on outliers. The boxplots will show the distribution of the data and highlight any remaining outliers after capping."
   ]
  },
  {
   "cell_type": "code",
   "id": "fde85eb79cf8ffb8",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "# Also created a simple boxplot of just the capped data\n",
    "def plot_capped_data_only(capped_df):\n",
    "\n",
    "    if 'ID' in numerical_cols:\n",
    "        numerical_cols.remove('ID')\n",
    "\n",
    "    n_cols = 3\n",
    "    n_plots = len(numerical_cols)\n",
    "    n_rows = (n_plots + n_cols - 1) // n_cols\n",
    "\n",
    "    plt.figure(figsize=(5 * n_cols, 4 * n_rows))\n",
    "\n",
    "    for i, col in enumerate(numerical_cols):\n",
    "        plt.subplot(n_rows, n_cols, i + 1)\n",
    "        sns.boxplot(x=capped_df[col], color='lightgreen')\n",
    "        plt.title(f'Capped Data: {col}')\n",
    "        plt.xlabel(col)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "print(\"Boxplots of capped data only:\")\n",
    "plot_capped_data_only(df_capped)\n",
    "\n",
    "# Check if the cleaned customer CSV exists\n",
    "if os.path.exists('cleaned_customer_segmentation.csv'):\n",
    "    print(f\"Existing cleaned_customer_segmentation.csv found, removing it...\")\n",
    "    os.remove('cleaned_customer_segmentation.csv')\n",
    "    print(\"File removed successfully.\")\n",
    "\n",
    "# Save the new cleaned dataset\n",
    "df_capped.to_csv('cleaned_customer_segmentation.csv', index=False)\n",
    "print(f\"New cleaned_customer_segmentation.csv saved successfully ({os.path.getsize('cleaned_customer_segmentation.csv')/1024:.2f} KB)\")\n",
    "\n",
    "# Verify the file was saved correctly\n",
    "if os.path.exists('cleaned_customer_segmentation.csv'):\n",
    "    # Load the cleaned dataset to confirm it was saved properly\n",
    "    df_cleaned = pd.read_csv('cleaned_customer_segmentation.csv')\n",
    "    print(f\"Loaded cleaned dataset with shape: {df_cleaned.shape}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "798242f0ff5a6728",
   "metadata": {},
   "source": [
    "## Feature Egineering\n",
    "Feature engineering is the process of using domain knowledge to extract features from raw data that make machine learning algorithms work. In this section, we will create new features based on the existing data in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "id": "8ec98410d33c528c",
   "metadata": {},
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "# Load data\n",
    "df = pd.read_csv('cleaned_customer_segmentation.csv')\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "11aa11abc93e206c",
   "metadata": {},
   "source": [
    "## Text encoding\n",
    "### Education\n",
    "- we chose ordinal encoding to put into perspective the increasing level of education\n",
    "1. first we will find the unique eduaction levels"
   ]
  },
  {
   "cell_type": "code",
   "id": "bed2eb288c8cf688",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "unique_eduaction_levels= df['Education'].unique()\n",
    "\n",
    "print(unique_eduaction_levels)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "45e143f4c4a12fe",
   "metadata": {},
   "source": [
    "## next steps\n",
    "2. supply ordinal encoder with oredered categories\n",
    "\n",
    "`['Basic', '2n Cycle', 'Graduation', 'Master', 'PhD']\n",
    "`\n",
    "\n",
    "3. fit the education column"
   ]
  },
  {
   "cell_type": "code",
   "id": "5f0b45857ae06112",
   "metadata": {},
   "source": [
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "\n",
    "ordered_education_categories = ['Basic', '2n Cycle', 'Graduation', 'Master', 'PhD']\n",
    "ordinal_encoder = OrdinalEncoder(categories=[ordered_education_categories])\n",
    "\n",
    "education_encoded = ordinal_encoder.fit_transform(df[['Education']])\n",
    "\n",
    "df['Education'] = education_encoded"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "89c5dbd3dcd149e1",
   "metadata": {},
   "source": [
    "\n",
    "print('Unique Marital Status:')\n",
    "print(df['Marital_Status'].unique())\n",
    "print('\\nMarital Status value counts:')\n",
    "print(df['Marital_Status'].value_counts())"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "2244b860c8ae5f43",
   "metadata": {},
   "source": [
    "## Marital status evaluation\n",
    "\n",
    "### Unique Marital Status:\n",
    "`['Single' 'Together' 'Married' 'Divorced' 'Widow' 'Alone' 'Absurd' 'YOLO']\n",
    "`\n",
    "### Marital Status value counts:\n",
    "`Marital_Status,\n",
    "Married     864,\n",
    "Together    580,\n",
    "Single      480,\n",
    "Divorced    232,\n",
    "Widow        77,\n",
    "Alone         3,\n",
    "Absurd        2`\n",
    "\n",
    "## Deduction\n",
    "### Wrong data\n",
    "- Yolo alone and absurd are wrong data entries.\n",
    "- Alone is the same as single and its values can be replaced with single\n",
    "- Absurd and yolo will be replaced with the mode\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "79263df768fd0b16",
   "metadata": {},
   "source": [
    "# replace Alone with Single\n",
    "df['Marital_Status'] = df['Marital_Status'].replace('Alone', 'Single')\n",
    "print('\\nMode: ',df['Marital_Status'].mode()[0])\n",
    "# Absurd and yolo  replaced with the mode\n",
    "df['Marital_Status'] = df['Marital_Status'].replace(['YOLO','Absurd'], df['Marital_Status'].mode()[0])\n",
    "\n",
    "#now check for the current state\n",
    "print('Unique Marital Status:')\n",
    "print(df['Marital_Status'].unique())\n",
    "print('\\nMarital Status value counts:')\n",
    "print(df['Marital_Status'].value_counts())\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "1de09e90d6c56034",
   "metadata": {},
   "source": [
    "### marital status encoding\n",
    "We went with **one hot encoding**,\n",
    "this is because ~~nominal encoding~~ would create a non existent _hierachy_"
   ]
  },
  {
   "cell_type": "code",
   "id": "3d35645ec74da013",
   "metadata": {},
   "source": [
    "# One-hot encode marital status\n",
    "marital_dummies = pd.get_dummies(df['Marital_Status'], prefix='Marital')\n",
    "df = pd.concat([df.drop('Marital_Status', axis=1), marital_dummies], axis=1)\n",
    "\n",
    "print(df.info())\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "fb564c002d7128fc",
   "metadata": {},
   "source": [
    "\n",
    "# Feature engineering with corrected date parsing\n",
    "try:\n",
    "\n",
    "    df['Dt_Customer'] = pd.to_datetime(df['Dt_Customer'], format='mixed', dayfirst=True)\n",
    "except ValueError:\n",
    "\n",
    "    df['Dt_Customer'] = pd.to_datetime(df['Dt_Customer'], format='%d-%m-%Y', errors='coerce')\n",
    "\n",
    "# Ensure Education is properly encoded as numbers\n",
    "print(\"Education column before encoding:\")\n",
    "print(f\"Data type: {df['Education'].dtype}\")\n",
    "print(f\"Unique values: {df['Education'].unique()}\")\n",
    "\n",
    "# If Education is still text, encode it properly\n",
    "if df['Education'].dtype == 'object':\n",
    "    education_mapping = {'Basic': 0, '2n Cycle': 1, 'Graduation': 2, 'Master': 3, 'PhD': 4}\n",
    "    df['Education'] = df['Education'].map(education_mapping)\n",
    "    print(\"Education encoded using mapping\")\n",
    "else:\n",
    "    print(\"Education already encoded\")\n",
    "\n",
    "print(\"Education column after encoding:\")\n",
    "print(f\"Data type: {df['Education'].dtype}\")\n",
    "print(f\"Unique values: {sorted(df['Education'].unique())}\")\n",
    "\n",
    "df['Age'] = 2025- df['Year_Birth']\n",
    "df['Total_Dependents'] = df['Kidhome'] + df['Teenhome']\n",
    "df['Is_Parent'] = (df['Total_Dependents'] > 0).astype(int)\n",
    "df['Total_Spending'] = df[['MntWines','MntFruits','MntMeatProducts','MntFishProducts','MntSweetProducts','MntGoldProds']].sum(axis=1)\n",
    "df['Total_Purchases'] = df[['NumWebPurchases','NumCatalogPurchases','NumStorePurchases']].sum(axis=1)\n",
    "df['Average_Spent'] = df['Total_Spending']/df['Total_Purchases'].replace(0,1)\n",
    "df['Tenure_Days'] = (pd.to_datetime('today') - df['Dt_Customer']).dt.days\n",
    "df['Total_Accepted_Cmp']=df[['AcceptedCmp1','AcceptedCmp2','AcceptedCmp3','AcceptedCmp4','AcceptedCmp5']].sum(axis=1)\n",
    "df['Web_Ratio'] = df['NumWebPurchases'] / df['Total_Purchases']\n",
    "df['Store_Ratio'] = df['NumStorePurchases'] / df['Total_Purchases']\n",
    "df['Catalog_Ratio'] = df['NumCatalogPurchases'] / df['Total_Purchases']\n",
    "df['Deals_Ratio'] = df['NumDealsPurchases'] / df['Total_Purchases']\n",
    "df['Ever_Accepted_Campaign'] = (df['Total_Accepted_Cmp'] > 0).astype(int)\n",
    "\n",
    "# Verify date conversion\n",
    "print(\"\\nDate conversion samples:\")\n",
    "print(df[['Dt_Customer', 'Tenure_Days']].head())\n",
    "\n",
    "# Drop unnecessary columns\n",
    "# df = df.drop(['ID', 'Year_Birth', 'Z_CostContact', 'Z_Revenue','Complain','Response','Kidhome', 'Teenhome','AcceptedCmp1','MntWines','MntFruits','MntMeatProducts','MntFishProducts','MntSweetProducts','MntGoldProds','AcceptedCmp2','AcceptedCmp3','AcceptedCmp4','AcceptedCmp5','MntWines','MntFruits','MntMeatProducts','MntFishProducts','MntSweetProducts','MntGoldProds'], axis=1, errors='ignore')\n",
    "\n",
    "# Check if the featured customer CSV exists\n",
    "if os.path.exists('featured_customer_segmentation.csv'):\n",
    "    print(f\"Existing featured_customer_segmentation.csv found, removing it...\")\n",
    "    os.remove('featured_customer_segmentation.csv')\n",
    "    print(\"File removed successfully.\")\n",
    "\n",
    "# Save the new featured dataset\n",
    "df.to_csv('featured_customer_segmentation.csv', index=False)\n",
    "print(f\"New featured_customer_segmentation.csv saved successfully ({os.path.getsize('featured_customer_segmentation.csv')/1024:.2f} KB)\")\n",
    "\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "1e3dff42",
   "metadata": {},
   "source": [
    "# Feature Engineering Evaluation and Analysis\n",
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.cluster import KMeans\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv('featured_customer_segmentation.csv')\n",
    "\n",
    "print(\"Dataset Analysis\")\n",
    "print(\"-\" * 40)\n",
    "print(f\"Shape: {df.shape}\")\n",
    "print(f\"Columns: {len(df.columns)}\")\n",
    "\n",
    "# Data preparation function\n",
    "def prepare_clustering_data(data, feature_list):\n",
    "    # Check which features actually exist\n",
    "    valid_features = [f for f in feature_list if f in data.columns]\n",
    "    missing_features = [f for f in feature_list if f not in data.columns]\n",
    "    \n",
    "    if missing_features:\n",
    "        print(f\"Missing: {missing_features}\")\n",
    "    \n",
    "    X = data[valid_features].copy()\n",
    "    \n",
    "    # Handle different data types\n",
    "    for col in X.columns:\n",
    "        if X[col].dtype == 'object':\n",
    "            if col == 'Education':\n",
    "                # Map education levels\n",
    "                edu_map = {'Basic': 0, '2n Cycle': 1, 'Graduation': 2, 'Master': 3, 'PhD': 4}\n",
    "                X[col] = X[col].map(edu_map)\n",
    "            else:\n",
    "                # Label encode other categorical variables\n",
    "                le = LabelEncoder()\n",
    "                X[col] = le.fit_transform(X[col].astype(str))\n",
    "        \n",
    "        # Fill missing values with mean\n",
    "        if X[col].dtype in ['int64', 'float64'] and X[col].isnull().sum() > 0:\n",
    "            X[col] = X[col].fillna(X[col].mean())\n",
    "    \n",
    "    # Convert everything to numeric\n",
    "    for col in X.columns:\n",
    "        if X[col].dtype not in ['int64', 'float64']:\n",
    "            X[col] = pd.to_numeric(X[col], errors='coerce')\n",
    "            X[col] = X[col].fillna(X[col].mean())\n",
    "    \n",
    "    return X\n",
    "\n",
    "# Define feature sets\n",
    "business_metrics = [\n",
    "    'Age', 'Total_Dependents', 'Is_Parent', \n",
    "    'Total_Spending', 'Total_Purchases', 'Average_Spent', \n",
    "    'Tenure_Days', 'Total_Accepted_Cmp', \n",
    "    'Ever_Accepted_Campaign', 'Income'\n",
    "]\n",
    "\n",
    "# Check available features\n",
    "available_features = [f for f in business_metrics if f in df.columns]\n",
    "print(f\"\\nUsing features: {available_features}\")\n",
    "\n",
    "# Clustering evaluation\n",
    "def run_clustering_analysis(data, features, analysis_name):\n",
    "    print(f\"\\n{analysis_name}\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    X = prepare_clustering_data(data, features)\n",
    "    \n",
    "    if X.empty:\n",
    "        print(\"No valid features found\")\n",
    "        return None\n",
    "    \n",
    "    # Scale the data\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(X)\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    # Test different cluster numbers\n",
    "    for k in range(2, 8):\n",
    "        kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)\n",
    "        labels = kmeans.fit_predict(X_scaled)\n",
    "        \n",
    "        sil_score = silhouette_score(X_scaled, labels)\n",
    "        inertia = kmeans.inertia_\n",
    "        \n",
    "        results.append({\n",
    "            'k': k,\n",
    "            'silhouette': sil_score,\n",
    "            'inertia': inertia\n",
    "        })\n",
    "        \n",
    "        print(f\"K={k}: Silhouette={sil_score:.4f}, Inertia={inertia:.0f}\")\n",
    "    \n",
    "    # Find best result\n",
    "    best_result = max(results, key=lambda x: x['silhouette'])\n",
    "    print(f\"\\nBest: K={best_result['k']}, Score={best_result['silhouette']:.4f}\")\n",
    "    \n",
    "    return {\n",
    "        'results': results,\n",
    "        'best_k': best_result['k'],\n",
    "        'best_score': best_result['silhouette'],\n",
    "        'feature_count': len(X.columns)\n",
    "    }\n",
    "\n",
    "# Run the analysis\n",
    "analysis_results = run_clustering_analysis(df, available_features, \"Business Feature Analysis\")\n",
    "\n",
    "if analysis_results:\n",
    "    print(f\"\\nSummary\")\n",
    "    print(\"-\" * 30)\n",
    "    print(f\"Best cluster count: {analysis_results['best_k']}\")\n",
    "    print(f\"Silhouette score: {analysis_results['best_score']:.4f}\")\n",
    "    print(f\"Features used: {analysis_results['feature_count']}\")\n",
    "    print(f\"Interpretable: {'Yes' if analysis_results['feature_count'] <= 10 else 'No'}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "af868a66",
   "metadata": {},
   "source": [
    "# Data Visualization and Summary\n",
    "print(\"\\nData Visualization\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "# Show all columns\n",
    "print(\"Available columns:\")\n",
    "for i, col in enumerate(df.columns, 1):\n",
    "    print(f\"{i:2d}. {col}\")\n",
    "\n",
    "# Select features for visualization\n",
    "viz_features = []\n",
    "target_features = ['Age', 'Total_Dependents', 'Total_Spending', 'Total_Purchases',\n",
    "                   'Average_Spent', 'Tenure_Days', 'Income', 'Education', \n",
    "                   'Is_Parent', 'Ever_Accepted_Campaign']\n",
    "\n",
    "for feature in target_features:\n",
    "    if feature in df.columns:\n",
    "        viz_features.append(feature)\n",
    "\n",
    "print(f\"\\nVisualizing: {len(viz_features)} features\")\n",
    "\n",
    "# Create distribution plots\n",
    "if len(viz_features) > 0:\n",
    "    n_plots = min(12, len(viz_features))\n",
    "    \n",
    "    fig, axes = plt.subplots(3, 4, figsize=(20, 15))\n",
    "    fig.suptitle('Feature Distributions', fontsize=16, y=0.98)\n",
    "    \n",
    "    for idx, feature in enumerate(viz_features[:n_plots]):\n",
    "        row = idx // 4\n",
    "        col = idx % 4\n",
    "        ax = axes[row, col]\n",
    "        \n",
    "        try:\n",
    "            sns.histplot(data=df, x=feature, kde=True, ax=ax, alpha=0.7)\n",
    "            ax.set_title(f'{feature}')\n",
    "            ax.grid(True, alpha=0.3)\n",
    "            \n",
    "            # Add mean and median lines\n",
    "            mean_val = df[feature].mean()\n",
    "            median_val = df[feature].median()\n",
    "            ax.axvline(mean_val, color='red', linestyle='--', alpha=0.8, linewidth=1)\n",
    "            ax.axvline(median_val, color='green', linestyle='--', alpha=0.8, linewidth=1)\n",
    "            \n",
    "        except Exception as e:\n",
    "            ax.text(0.5, 0.5, f'Error: {feature}', ha='center', va='center', transform=ax.transAxes)\n",
    "    \n",
    "    # Remove empty subplots\n",
    "    for idx in range(n_plots, 12):\n",
    "        row = idx // 4\n",
    "        col = idx % 4\n",
    "        if row < 3 and col < 4:\n",
    "            axes[row, col].remove()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Correlation analysis\n",
    "corr_features = ['Age', 'Total_Dependents', 'Total_Spending', 'Total_Purchases', \n",
    "                'Average_Spent', 'Tenure_Days', 'Income', 'Education']\n",
    "\n",
    "available_corr = [f for f in corr_features if f in df.columns]\n",
    "\n",
    "if len(available_corr) > 1:\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    \n",
    "    corr_matrix = df[available_corr].corr()\n",
    "    mask = np.triu(np.ones_like(corr_matrix, dtype=bool))\n",
    "    \n",
    "    sns.heatmap(corr_matrix, mask=mask, annot=True, cmap='coolwarm', center=0,\n",
    "                square=True, linewidths=0.5, cbar_kws={\"shrink\": .8})\n",
    "    plt.title('Feature Correlations', fontsize=14, pad=20)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Dataset summary\n",
    "print(\"\\nDataset Summary\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Total customers: {len(df):,}\")\n",
    "\n",
    "# Basic statistics\n",
    "if 'Age' in df.columns:\n",
    "    print(f\"Age range: {df['Age'].min():.0f} - {df['Age'].max():.0f} years\")\n",
    "if 'Total_Dependents' in df.columns:\n",
    "    print(f\"Average dependents: {df['Total_Dependents'].mean():.1f}\")\n",
    "if 'Is_Parent' in df.columns:\n",
    "    parents = df['Is_Parent'].sum()\n",
    "    print(f\"Parents: {parents:,} ({parents/len(df)*100:.1f}%)\")\n",
    "if 'Total_Spending' in df.columns:\n",
    "    print(f\"Average spending: ${df['Total_Spending'].mean():.0f}\")\n",
    "if 'Tenure_Days' in df.columns:\n",
    "    print(f\"Average tenure: {df['Tenure_Days'].mean():.0f} days\")\n",
    "if 'Ever_Accepted_Campaign' in df.columns:\n",
    "    responders = df['Ever_Accepted_Campaign'].sum()\n",
    "    print(f\"Campaign responders: {responders:,} ({responders/len(df)*100:.1f}%)\")\n",
    "\n",
    "# Channel analysis\n",
    "channel_cols = ['Web_Ratio', 'Store_Ratio', 'Catalog_Ratio', 'Deals_Ratio']\n",
    "available_channels = [c for c in channel_cols if c in df.columns]\n",
    "\n",
    "if available_channels:\n",
    "    print(f\"\\nChannel Usage:\")\n",
    "    for channel in available_channels:\n",
    "        print(f\"{channel}: {df[channel].mean():.3f}\")\n",
    "\n",
    "print(f\"\\nKey Insights:\")\n",
    "print(f\"- Created business-focused metrics from raw transaction data\")\n",
    "print(f\"- Simplified feature set for better interpretability\")\n",
    "print(f\"- Ready for customer segmentation analysis\")\n",
    "print(f\"- Features capture customer lifecycle and behavior patterns\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "6b742d06",
   "metadata": {},
   "source": [
    "# Fixed Comprehensive Feature Engineering Evaluation\n",
    "\n",
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.cluster import KMeans\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Load both datasets\n",
    "df_original = pd.read_csv('cleaned_customer_segmentation.csv')\n",
    "df_featured = pd.read_csv('featured_customer_segmentation.csv')\n",
    "\n",
    "print(\"=== COMPREHENSIVE FEATURE ENGINEERING EVALUATION ===\\n\")\n",
    "print(f\"Original dataset columns: {list(df_original.columns)}\")\n",
    "print(f\"Featured dataset columns: {list(df_featured.columns)}\")\n",
    "\n",
    "# Function to safely prepare data for clustering\n",
    "def prepare_data_safely(data, features):\n",
    "    \"\"\"Prepare data by handling text columns and missing values\"\"\"\n",
    "    available_features = [f for f in features if f in data.columns]\n",
    "    missing_features = [f for f in features if f not in data.columns]\n",
    "    \n",
    "    if missing_features:\n",
    "        print(f\"Warning: Missing features: {missing_features}\")\n",
    "    \n",
    "    X = data[available_features].copy()\n",
    "    print(f\"Available features: {available_features}\")\n",
    "    \n",
    "    # Handle each column based on its data type\n",
    "    for col in X.columns:\n",
    "        if X[col].dtype == 'object':\n",
    "            # Handle text columns\n",
    "            if col == 'Education':\n",
    "                # Use specific mapping for education\n",
    "                education_map = {'Basic': 0, '2n Cycle': 1, 'Graduation': 2, 'Master': 3, 'PhD': 4}\n",
    "                X[col] = X[col].map(education_map)\n",
    "                print(f\"Mapped Education: {X[col].unique()}\")\n",
    "            else:\n",
    "                # Use label encoding for other text columns\n",
    "                le = LabelEncoder()\n",
    "                X[col] = le.fit_transform(X[col].astype(str))\n",
    "                print(f\"Label encoded {col}: {X[col].unique()}\")\n",
    "        \n",
    "        # Handle missing values for numeric columns\n",
    "        if X[col].dtype in ['int64', 'float64']:\n",
    "            missing_count = X[col].isnull().sum()\n",
    "            if missing_count > 0:\n",
    "                X[col] = X[col].fillna(X[col].mean())\n",
    "                print(f\"Filled {missing_count} missing values in {col}\")\n",
    "    \n",
    "    # Final verification - ensure all columns are numeric\n",
    "    for col in X.columns:\n",
    "        if X[col].dtype not in ['int64', 'float64']:\n",
    "            print(f\"Warning: {col} is still not numeric: {X[col].dtype}\")\n",
    "            X[col] = pd.to_numeric(X[col], errors='coerce')\n",
    "            X[col] = X[col].fillna(X[col].mean())\n",
    "    \n",
    "    print(f\"Final data shape: {X.shape}\")\n",
    "    return X\n",
    "\n",
    "# Define feature sets with CORRECT column names\n",
    "original_features = [\n",
    "    'Income', 'MntWines', 'MntFruits', 'MntMeatProducts', \n",
    "    'MntFishProducts', 'MntSweetProducts', 'MntGoldProds',\n",
    "    'NumWebPurchases', 'NumCatalogPurchases', 'NumStorePurchases',\n",
    "    'NumDealsPurchases', 'NumWebVisitsMonth',\n",
    "    'Education', 'Recency', 'Kidhome', 'Teenhome'\n",
    "]\n",
    "\n",
    "business_features = [\n",
    "    'Age', 'Total_Dependents', 'Is_Parent', \n",
    "    'Total_Spending', 'Total_Purchases', 'Average_Spent', \n",
    "    'Tenure_Days', 'Total_Accepted_Cmp', \n",
    "    'Ever_Accepted_Campaign', 'Income'\n",
    "]\n",
    "\n",
    "# Fixed channel features with correct column names\n",
    "channel_features = [\n",
    "    'Web_Ratio', 'Store_Ratio', 'Catalog_Ratio', 'Deals_Ratio',  # Fixed names\n",
    "    'Total_Purchases', 'Average_Spent', 'Income', 'Age'\n",
    "]\n",
    "\n",
    "# Function to evaluate clustering with error handling\n",
    "def evaluate_comprehensive(data, features, name, k_range=range(2, 8)):\n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(f\"{name} Analysis:\")\n",
    "    print(f\"{'='*50}\")\n",
    "    \n",
    "    try:\n",
    "        X = prepare_data_safely(data, features)\n",
    "        \n",
    "        if X.empty or len(X.columns) == 0:\n",
    "            print(f\"Error: No valid features for {name}\")\n",
    "            return create_empty_results()\n",
    "        \n",
    "        # Scale features\n",
    "        scaler = StandardScaler()\n",
    "        X_scaled = scaler.fit_transform(X)\n",
    "        \n",
    "        silhouette_scores = []\n",
    "        inertias = []\n",
    "        \n",
    "        print(f\"\\nClustering results for {name}:\")\n",
    "        for k in k_range:\n",
    "            kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)\n",
    "            cluster_labels = kmeans.fit_predict(X_scaled)\n",
    "            \n",
    "            sil_score = silhouette_score(X_scaled, cluster_labels)\n",
    "            inertia = kmeans.inertia_\n",
    "            \n",
    "            silhouette_scores.append(sil_score)\n",
    "            inertias.append(inertia)\n",
    "            \n",
    "            print(f\"K={k}: Silhouette={sil_score:.4f}, Inertia={inertia:.0f}\")\n",
    "        \n",
    "        # Find best configuration\n",
    "        best_sil_idx = np.argmax(silhouette_scores)\n",
    "        best_k = list(k_range)[best_sil_idx]\n",
    "        best_score = silhouette_scores[best_sil_idx]\n",
    "        \n",
    "        print(f\"\\n🏆 Best result: K={best_k}, Silhouette={best_score:.4f}\")\n",
    "        \n",
    "        return {\n",
    "            'silhouette_scores': silhouette_scores,\n",
    "            'inertias': inertias,\n",
    "            'best_k': best_k,\n",
    "            'best_score': best_score,\n",
    "            'feature_count': len(X.columns),\n",
    "            'interpretability': len(X.columns) <= 10,\n",
    "            'success': True,\n",
    "            'features_used': list(X.columns)\n",
    "        }\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\" Error in {name}: {str(e)}\")\n",
    "        return create_empty_results()\n",
    "\n",
    "def create_empty_results():\n",
    "    return {\n",
    "        'silhouette_scores': [],\n",
    "        'inertias': [],\n",
    "        'best_k': 0,\n",
    "        'best_score': 0,\n",
    "        'feature_count': 0,\n",
    "        'interpretability': False,\n",
    "        'success': False,\n",
    "        'features_used': []\n",
    "    }\n",
    "\n",
    "# Evaluate all approaches\n",
    "print(\" EVALUATING DIFFERENT FEATURE APPROACHES\")\n",
    "original_results = evaluate_comprehensive(df_original, original_features, \"ORIGINAL FEATURES\")\n",
    "business_results = evaluate_comprehensive(df_featured, business_features, \"BUSINESS-FOCUSED FEATURES\")\n",
    "channel_results = evaluate_comprehensive(df_featured, channel_features, \"CHANNEL-FOCUSED FEATURES\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "4c6e55d0",
   "metadata": {},
   "source": [
    "# Comprehensive Feature Engineering Evaluation with Better Error Handling\n",
    "\n",
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.cluster import KMeans\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load both datasets\n",
    "df_original = pd.read_csv('cleaned_customer_segmentation.csv')\n",
    "df_featured = pd.read_csv('featured_customer_segmentation.csv')\n",
    "\n",
    "print(\"=== COMPREHENSIVE FEATURE ENGINEERING EVALUATION ===\\n\")\n",
    "\n",
    "# Function to safely prepare data for clustering\n",
    "def prepare_data_safely(data, features):\n",
    "    \"\"\"Prepare data by handling text columns and missing values\"\"\"\n",
    "    available_features = [f for f in features if f in data.columns]\n",
    "    X = data[available_features].copy()\n",
    "    \n",
    "    print(f\"Available features: {available_features}\")\n",
    "    \n",
    "    # Handle each column based on its data type\n",
    "    for col in X.columns:\n",
    "        print(f\"Processing {col}: {X[col].dtype}\")\n",
    "        \n",
    "        if X[col].dtype == 'object':\n",
    "            # Handle text columns\n",
    "            if col == 'Education':\n",
    "                # Use specific mapping for education\n",
    "                education_map = {'Basic': 0, '2n Cycle': 1, 'Graduation': 2, 'Master': 3, 'PhD': 4}\n",
    "                X[col] = X[col].map(education_map)\n",
    "                print(f\"Mapped Education: {X[col].unique()}\")\n",
    "            else:\n",
    "                # Use label encoding for other text columns\n",
    "                le = LabelEncoder()\n",
    "                X[col] = le.fit_transform(X[col].astype(str))\n",
    "                print(f\"Label encoded {col}: {X[col].unique()}\")\n",
    "        \n",
    "        # Handle missing values for numeric columns\n",
    "        if X[col].dtype in ['int64', 'float64']:\n",
    "            missing_count = X[col].isnull().sum()\n",
    "            if missing_count > 0:\n",
    "                X[col] = X[col].fillna(X[col].mean())\n",
    "                print(f\"Filled {missing_count} missing values in {col}\")\n",
    "    \n",
    "    # Final verification - ensure all columns are numeric\n",
    "    for col in X.columns:\n",
    "        if X[col].dtype not in ['int64', 'float64']:\n",
    "            print(f\"Warning: {col} is still not numeric: {X[col].dtype}\")\n",
    "            X[col] = pd.to_numeric(X[col], errors='coerce')\n",
    "            X[col] = X[col].fillna(X[col].mean())\n",
    "    \n",
    "    print(f\"Final data shape: {X.shape}\")\n",
    "    print(f\"Final data types:\\n{X.dtypes}\")\n",
    "    \n",
    "    return X\n",
    "\n",
    "# Define feature sets\n",
    "original_features = [\n",
    "    'Income', 'MntWines', 'MntFruits', 'MntMeatProducts', \n",
    "    'MntFishProducts', 'MntSweetProducts', 'MntGoldProds',\n",
    "    'NumWebPurchases', 'NumCatalogPurchases', 'NumStorePurchases',\n",
    "    'NumDealsPurchases', 'NumWebVisitsMonth',\n",
    "    'Education', 'Recency', 'Kidhome', 'Teenhome'\n",
    "]\n",
    "\n",
    "business_features = [\n",
    "    'Age', 'Total_Dependents', 'Is_Parent', \n",
    "    'Total_Spending', 'Total_Purchases', 'Average_Spent', \n",
    "    'Tenure_Days', 'Total_Accepted_Cmp', \n",
    "    'Ever_Accepted_Campaign', 'Income'\n",
    "]\n",
    "\n",
    "channel_features = [\n",
    "    'Web_Ratio', 'Store_Ratio', 'Catalog_Ratio', 'Deals_Ratio',\n",
    "    'Total_Purchases', 'Average_Spent', 'Income', 'Age'\n",
    "]\n",
    "\n",
    "# Function to evaluate clustering with error handling\n",
    "def evaluate_comprehensive(data, features, name, k_range=range(2, 8)):\n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(f\"{name} Analysis:\")\n",
    "    print(f\"{'='*50}\")\n",
    "    \n",
    "    try:\n",
    "        X = prepare_data_safely(data, features)\n",
    "        \n",
    "        if X.empty or len(X.columns) == 0:\n",
    "            print(f\"Error: No valid features for {name}\")\n",
    "            return create_empty_results()\n",
    "        \n",
    "        # Scale features\n",
    "        scaler = StandardScaler()\n",
    "        X_scaled = scaler.fit_transform(X)\n",
    "        \n",
    "        silhouette_scores = []\n",
    "        inertias = []\n",
    "        \n",
    "        print(f\"\\nClustering results for {name}:\")\n",
    "        for k in k_range:\n",
    "            kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)\n",
    "            cluster_labels = kmeans.fit_predict(X_scaled)\n",
    "            \n",
    "            sil_score = silhouette_score(X_scaled, cluster_labels)\n",
    "            inertia = kmeans.inertia_\n",
    "            \n",
    "            silhouette_scores.append(sil_score)\n",
    "            inertias.append(inertia)\n",
    "            \n",
    "            print(f\"K={k}: Silhouette={sil_score:.4f}, Inertia={inertia:.0f}\")\n",
    "        \n",
    "        # Find best configuration\n",
    "        best_sil_idx = np.argmax(silhouette_scores)\n",
    "        best_k = list(k_range)[best_sil_idx]\n",
    "        best_score = silhouette_scores[best_sil_idx]\n",
    "        \n",
    "        print(f\"\\n Best result: K={best_k}, Silhouette={best_score:.4f}\")\n",
    "        \n",
    "        return {\n",
    "            'silhouette_scores': silhouette_scores,\n",
    "            'inertias': inertias,\n",
    "            'best_k': best_k,\n",
    "            'best_score': best_score,\n",
    "            'feature_count': len(X.columns),\n",
    "            'interpretability': len(X.columns) <= 10,\n",
    "            'success': True\n",
    "        }\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\" Error in {name}: {str(e)}\")\n",
    "        return create_empty_results()\n",
    "\n",
    "def create_empty_results():\n",
    "    return {\n",
    "        'silhouette_scores': [],\n",
    "        'inertias': [],\n",
    "        'best_k': 0,\n",
    "        'best_score': 0,\n",
    "        'feature_count': 0,\n",
    "        'interpretability': False,\n",
    "        'success': False\n",
    "    }\n",
    "\n",
    "# Evaluate all approaches\n",
    "print(\" EVALUATING DIFFERENT FEATURE APPROACHES\")\n",
    "original_results = evaluate_comprehensive(df_original, original_features, \"ORIGINAL FEATURES\")\n",
    "business_results = evaluate_comprehensive(df_featured, business_features, \"BUSINESS-FOCUSED FEATURES\")\n",
    "channel_results = evaluate_comprehensive(df_featured, channel_features, \"CHANNEL-FOCUSED FEATURES\")\n",
    "\n",
    "# Create summary\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\" FEATURE ENGINEERING IMPACT ANALYSIS\")\n",
    "print(f\"{'='*80}\")\n",
    "\n",
    "results_summary = {\n",
    "    'Original Features': original_results,\n",
    "    'Business Features': business_results,\n",
    "    'Channel Features': channel_results\n",
    "}\n",
    "\n",
    "# Display results table\n",
    "print(f\"{'Approach':<20} {'Status':<10} {'Best K':<8} {'Silhouette':<12} {'Features':<10} {'Interpretable':<15}\")\n",
    "print(\"-\" * 85)\n",
    "\n",
    "successful_results = {}\n",
    "for approach, results in results_summary.items():\n",
    "    if results['success']:\n",
    "        interpretable = \" Yes\" if results['interpretability'] else \" No\"\n",
    "        print(f\"{approach:<20} {' Success':<10} {results['best_k']:<8} {results['best_score']:<12.4f} {results['feature_count']:<10} {interpretable:<15}\")\n",
    "        successful_results[approach] = results\n",
    "    else:\n",
    "        print(f\"{approach:<20} {' Failed':<10} {'N/A':<8} {'N/A':<12} {'N/A':<10} {'N/A':<15}\")\n",
    "\n",
    "# Business value analysis\n",
    "if len(successful_results) >= 2:\n",
    "    print(f\"\\n BUSINESS VALUE IMPROVEMENTS:\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    if 'Original Features' in successful_results and 'Business Features' in successful_results:\n",
    "        original_count = successful_results['Original Features']['feature_count']\n",
    "        business_count = successful_results['Business Features']['feature_count']\n",
    "        reduction = ((original_count - business_count) / original_count) * 100\n",
    "        print(f\" Feature Simplification: {reduction:.1f}% reduction in features\")\n",
    "    \n",
    "    print(f\" Interpretability: Business features are more actionable\")\n",
    "    print(f\" Domain Knowledge: Engineered features reflect business understanding\")\n",
    "    \n",
    "    # Show best performing approach\n",
    "    best_approach = max(successful_results.items(), key=lambda x: x[1]['best_score'])\n",
    "    print(f\"Best Performance: {best_approach[0]} (Silhouette: {best_approach[1]['best_score']:.4f})\")\n",
    "    \n",
    "    # Visualization for successful results\n",
    "    if len(successful_results) >= 2:\n",
    "        fig, axes = plt.subplots(1, 2, figsize=(15, 6))\n",
    "        \n",
    "        k_range = range(2, 8)\n",
    "        colors = ['red', 'green', 'blue']\n",
    "        markers = ['o', 's', '^']\n",
    "        \n",
    "        # Silhouette comparison\n",
    "        for i, (name, results) in enumerate(successful_results.items()):\n",
    "            if results['silhouette_scores']:\n",
    "                short_name = name.split()[0]  # Get first word\n",
    "                axes[0].plot(k_range, results['silhouette_scores'], \n",
    "                           f'{markers[i]}-', label=f'{short_name} ({results[\"feature_count\"]} features)', \n",
    "                           linewidth=2, markersize=8, color=colors[i])\n",
    "        \n",
    "        axes[0].set_xlabel('Number of Clusters (K)')\n",
    "        axes[0].set_ylabel('Silhouette Score')\n",
    "        axes[0].set_title('Silhouette Score Comparison')\n",
    "        axes[0].legend()\n",
    "        axes[0].grid(True, alpha=0.3)\n",
    "        \n",
    "        # Feature count comparison\n",
    "        names = [name.split()[0] for name in successful_results.keys()]\n",
    "        counts = [results['feature_count'] for results in successful_results.values()]\n",
    "        scores = [results['best_score'] for results in successful_results.values()]\n",
    "        \n",
    "        bars = axes[1].bar(names, scores, color=colors[:len(names)], alpha=0.7)\n",
    "        axes[1].set_ylabel('Best Silhouette Score')\n",
    "        axes[1].set_title('Best Performance Comparison')\n",
    "        axes[1].grid(True, alpha=0.3)\n",
    "        \n",
    "        # Add value labels\n",
    "        for bar, score in zip(bars, scores):\n",
    "            height = bar.get_height()\n",
    "            axes[1].text(bar.get_x() + bar.get_width()/2., height + 0.001,\n",
    "                       f'{score:.3f}', ha='center', va='bottom', fontweight='bold')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "print(f\"\\nBUSINESS IMPACT OF FEATURE ENGINEERING:\")\n",
    "print(f\"{'='*60}\")\n",
    "print(f\" Actionable Insights: Simplified features enable clear business actions\")\n",
    "print(f\" Better Targeting: Age, family status, and spending patterns for marketing\")\n",
    "print(f\" Channel Optimization: Purchase ratios guide channel investment\")\n",
    "print(f\" Revenue Impact: Customer lifetime value insights from tenure and spending\")\n",
    "print(f\" Campaign Efficiency: Campaign response patterns for better ROI\")\n",
    "print(f\" Operational Efficiency: Fewer features = faster analysis and deployment\")\n",
    "\n",
    "print(f\"\\n KEY FEATURE ENGINEERING WINS:\")\n",
    "print(f\"{'='*60}\")\n",
    "print(f\" Created interpretable business metrics from raw data\")\n",
    "print(f\" Enabled customer lifecycle analysis (Age, Tenure)\")\n",
    "print(f\" Introduced family-based segmentation (Total_Dependents, Is_Parent)\")\n",
    "print(f\" Built channel preference indicators for omnichannel strategy\")\n",
    "print(f\" Consolidated spending behavior into actionable metrics\")\n",
    "print(f\" Resolved data encoding and quality issues\")"
   ],
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
