{
 "cells": [
  {
   "cell_type": "code",
   "id": "fa38c842",
   "metadata": {},
   "source": [
    "from enum import unique\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "# import matplotlib as mpl\n",
    "import seaborn as sns\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# from sklearn.preprocessing import LabelEncoder,StandardScaler\n",
    "# from  sklearn.feature_extraction.text import CountVectorizer\n",
    "# from sklearn import metrics\n",
    "# from xgboost import XGBRegressor\n",
    "# from datetime import datetime\n",
    "\n",
    "\n",
    "\n",
    "import warnings\n",
    "\n",
    "from fontTools.subset import subset\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "bf6a6c33",
   "metadata": {},
   "source": [
    "df = pd.read_csv('Customer_Segmentation.csv')\n",
    "df.head()\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Finding missing values",
   "id": "211880a254938290"
  },
  {
   "cell_type": "code",
   "id": "b41ae272",
   "metadata": {},
   "source": [
    "counts_missing = df.isnull().sum()\n",
    "missing_percentage = (counts_missing / len(df)) * 100\n",
    "\n",
    "#summary of missing values\n",
    "missing_summary = pd.DataFrame({\n",
    "    'Missing Values': counts_missing,\n",
    "    'Percentage': missing_percentage\n",
    "}).reset_index().rename(columns={'index': 'Column'})\n",
    "print(missing_summary)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Filling missing data\n",
    "## Income\n",
    "we have 24 missing values in the Income column, which is significant. We will handle this by using median imputation.\n",
    "\n",
    "1. Median is robust to outliers, which is important for income data that may have extreme values.\n",
    "2. It preserves the distribution of the data better than mean imputation."
   ],
   "id": "cb49439ea0f7eab4"
  },
  {
   "cell_type": "code",
   "id": "1fb1133d",
   "metadata": {},
   "source": [
    "#we decided to use median impoutation for the missing values in Income column.This is because of the following reasons\n",
    "\n",
    "df['Income'].fillna(df['Income'].median(), inplace=True)\n",
    "print(df[\"Income\"].isnull().sum()) \n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Finding duplicates\n",
    "No duplicates found"
   ],
   "id": "20f3ceacc3a81010"
  },
  {
   "cell_type": "code",
   "id": "f5aa73f0",
   "metadata": {},
   "source": [
    "df.duplicated().sum()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Date Formatting\n",
    "Dt_Customer is in the format of 'dd-mm-yyyy', we will convert it to a datetime object for easier analysis."
   ],
   "id": "20bfdc33a8313fb9"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "df['Dt_Customer'] = pd.to_datetime(df['Dt_Customer'], format='%d-%m-%Y', errors='coerce')\n",
    "print(\"First 20 enrollment dates:\")\n",
    "print(df['Dt_Customer'].head(20))\n",
    "\n",
    "print('\\nNull vales after formatting: ')\n",
    "print(df[df['Dt_Customer'].isnull()])"
   ],
   "id": "70a154f35793cc6e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Checking for constant columns\n",
    "Constant columns are those that have the same value for all rows. These columns do not provide any useful information for analysis or modeling, so they can be safely removed."
   ],
   "id": "2f4fd0c33e1bd6d9"
  },
  {
   "cell_type": "code",
   "id": "4b4950b86b58828",
   "metadata": {},
   "source": [
    "\n",
    "constant_columns = [col for col in df.columns if df[col].nunique() == 1]\n",
    "print(constant_columns)\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Dropping constant\n",
    "\n",
    "### found 2\n",
    "- AcceptedCmp3\n",
    "- Z_Revenue\n",
    "\n",
    "We found 2 constant columns: 'AcceptedCmp3' and 'Z_Revenue'. These columns have the same value for all rows, which means they do not provide any useful information for analysis or modeling. Therefore, we will drop them from the dataset."
   ],
   "id": "246e203805fb78bf"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "\n",
    "# Remove constant columns\n",
    "df = df.drop(columns=constant_columns)\n",
    "\n",
    "print(f\"Removed {len(constant_columns)} constant columns: {constant_columns}\")"
   ],
   "id": "ec3b4243",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Finding Outliers\n",
    "Using the Interquartile Range (IQR) method to identify outliers in numerical columns. The IQR is a measure of statistical dispersion and is used to detect outliers by calculating the range between the first quartile (Q1) and the third quartile (Q3).\n"
   ],
   "id": "a115f2900e3f035a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "#identifying outliers using IQR method\n",
    "numerical_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "\n",
    "# Exclude ID and boolean columns\n",
    "bool_cols = [col for col in df.columns if np.all(np.isin(df[col].unique(), [0, 1]))]\n",
    "numerical_cols = [col for col in numerical_cols if col not in bool_cols and col != 'ID']\n",
    "# list of Outlier\n",
    "def outlier_summary(dataframe):\n",
    "\n",
    "    outlier_info = []\n",
    "    total_outliers = 0\n",
    "\n",
    "    for col in numerical_cols:\n",
    "        # Calculate IQR\n",
    "        Q1 = dataframe[col].quantile(0.25)\n",
    "        Q3 = dataframe[col].quantile(0.75)\n",
    "        IQR = Q3 - Q1\n",
    "\n",
    "        # Find outliers\n",
    "        lower_bound = Q1 - 1.5 * IQR\n",
    "        upper_bound = Q3 + 1.5 * IQR\n",
    "        outliers = dataframe[(dataframe[col] < lower_bound) | (dataframe[col] > upper_bound)]\n",
    "        outlier_count = len(outliers)\n",
    "\n",
    "        if outlier_count > 0:\n",
    "            outlier_percentage = (outlier_count / len(dataframe)) * 100\n",
    "            outlier_info.append((col, outlier_count, outlier_percentage))\n",
    "            total_outliers += outlier_count\n",
    "\n",
    "    print(f\"Columns with outliers: {len(outlier_info)}\")\n",
    "    print(f\"Total outliers: {total_outliers}\")\n",
    "\n",
    "    if outlier_info:\n",
    "        print(\"Columns with outliers:\")\n",
    "        print()\n",
    "        # Sort by outlier count (descending)\n",
    "        #outlier_info.sort(key=lambda x: x[1], reverse=True)\n",
    "\n",
    "        for col, count, percentage in outlier_info:\n",
    "            print(f\"{col}: {count} outliers ({percentage:.1f}%)\")\n",
    "    else:\n",
    "        print(\"No outliers detected in any column!\")\n",
    "\n",
    "# Run the outlier summary\n",
    "outlier_summary(df)\n"
   ],
   "id": "7bd936c0",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "#visualize the outliers using boxplots\n",
    "def plot_outliers(dataframe):\n",
    "    n_cols = 3\n",
    "    n_plots = len(numerical_cols)\n",
    "    n_rows = (n_plots + n_cols - 1) // n_cols  # Ceiling division\n",
    "\n",
    "    plt.figure(figsize=(5 * n_cols, 4 * n_rows))\n",
    "\n",
    "    for i, col in enumerate(numerical_cols):\n",
    "        plt.subplot(n_rows, n_cols, i + 1)\n",
    "        sns.boxplot(x=dataframe[col])\n",
    "        plt.title(f'Boxplot of {col}')\n",
    "        plt.xlabel(col)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# Run the outlier visualization\n",
    "plot_outliers(df)\n"
   ],
   "id": "ef50b76d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## handling the outliers\n",
    "# We will use the capping method to handle outliers. This involves replacing outliers with the nearest non-outlier value, which is determined by the IQR method.\n"
   ],
   "id": "dc60de1fea66aeaa"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Capping method to handle outliers\n",
    "def cap_outliers(dataframe):\n",
    "\n",
    "    df_capped = dataframe.copy()\n",
    "\n",
    "    for col in numerical_cols:\n",
    "        # Calculate IQR\n",
    "        Q1 = df_capped[col].quantile(0.25)\n",
    "        Q3 = df_capped[col].quantile(0.75)\n",
    "        IQR = Q3 - Q1\n",
    "        lower_bound = Q1 - 1.5 * IQR\n",
    "        upper_bound = Q3 + 1.5 * IQR\n",
    "\n",
    "        # Count outliers before capping\n",
    "        outliers_before = len(df_capped[(df_capped[col] < lower_bound) | (df_capped[col] > upper_bound)])\n",
    "\n",
    "        # Apply capping\n",
    "        df_capped[col] = df_capped[col].clip(lower=lower_bound, upper=upper_bound)\n",
    "\n",
    "        # Count outliers after capping\n",
    "        outliers_after = len(df_capped[(df_capped[col] < lower_bound) | (df_capped[col] > upper_bound)])\n",
    "\n",
    "        if outliers_before > 0:\n",
    "            print(f\"{col}: {outliers_before} outliers capped\")\n",
    "\n",
    "    print(f\"\\nOutlier capping completed!\")\n",
    "    return df_capped #return the capped DataFrame while preserving the original DataFrame\n",
    "\n",
    "# Apply capping to the dataset\n",
    "df_capped = cap_outliers(df)\n",
    "\n",
    "\n",
    "\n"
   ],
   "id": "30498ed8",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Visualizatin after capping\n",
    "# Visualize the capped data using boxplots\n",
    "We will create boxplots for each numerical column to visualize the effect of capping on outliers. The boxplots will show the distribution of the data and highlight any remaining outliers after capping."
   ],
   "id": "44c86dc14da84e37"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "\n",
    "\n",
    "# Also created a simple boxplot of just the capped data\n",
    "def plot_capped_data_only(capped_df):\n",
    "\n",
    "    if 'ID' in numerical_cols:\n",
    "        numerical_cols.remove('ID')\n",
    "\n",
    "    n_cols = 3\n",
    "    n_plots = len(numerical_cols)\n",
    "    n_rows = (n_plots + n_cols - 1) // n_cols\n",
    "\n",
    "    plt.figure(figsize=(5 * n_cols, 4 * n_rows))\n",
    "\n",
    "    for i, col in enumerate(numerical_cols):\n",
    "        plt.subplot(n_rows, n_cols, i + 1)\n",
    "        sns.boxplot(x=capped_df[col], color='lightgreen')\n",
    "        plt.title(f'Capped Data: {col}')\n",
    "        plt.xlabel(col)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "print(\"Boxplots of capped data only:\")\n",
    "plot_capped_data_only(df_capped)\n",
    "\n",
    "# Check if the cleaned customer CSV exists\n",
    "if os.path.exists('cleaned_customer_segmentation.csv'):\n",
    "    print(f\"Existing cleaned_customer_segmentation.csv found, removing it...\")\n",
    "    os.remove('cleaned_customer_segmentation.csv')\n",
    "    print(\"File removed successfully.\")\n",
    "\n",
    "# Save the new cleaned dataset\n",
    "df_capped.to_csv('cleaned_customer_segmentation.csv', index=False)\n",
    "print(f\"New cleaned_customer_segmentation.csv saved successfully ({os.path.getsize('cleaned_customer_segmentation.csv')/1024:.2f} KB)\")\n",
    "\n",
    "# Verify the file was saved correctly\n",
    "if os.path.exists('cleaned_customer_segmentation.csv'):\n",
    "    # Load the cleaned dataset to confirm it was saved properly\n",
    "    df_cleaned = pd.read_csv('cleaned_customer_segmentation.csv')\n",
    "    print(f\"Loaded cleaned dataset with shape: {df_cleaned.shape}\")"
   ],
   "id": "45ce69ec",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Feature Egineering\n",
    "Feature engineering is the process of using domain knowledge to extract features from raw data that make machine learning algorithms work. In this section, we will create new features based on the existing data in the dataset."
   ],
   "id": "e2ed597243206282"
  },
  {
   "cell_type": "code",
   "id": "9381fb40",
   "metadata": {},
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "# Load data\n",
    "df = pd.read_csv('cleaned_customer_segmentation.csv')\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Text encoding\n",
    "### Education\n",
    "- we chose ordinal encoding to put into perspective the increasing level of education\n",
    "1. first we will find the unique eduaction levels"
   ],
   "id": "66ed22fdc14891f0"
  },
  {
   "cell_type": "code",
   "id": "e7e186e21b2c6792",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "unique_eduaction_levels= df['Education'].unique()\n",
    "\n",
    "print(unique_eduaction_levels)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## next steps\n",
    "2. supply ordinal encoder with oredered categories\n",
    "\n",
    "`['Basic', '2n Cycle', 'Graduation', 'Master', 'PhD']\n",
    "`\n",
    "\n",
    "3. fit the education column"
   ],
   "id": "343bda9bb753d33e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "\n",
    "ordered_education_categories = ['Basic', '2n Cycle', 'Graduation', 'Master', 'PhD']\n",
    "ordinal_encoder = OrdinalEncoder(categories=[ordered_education_categories])\n",
    "\n",
    "education_encoded = ordinal_encoder.fit_transform(df[['Education']])\n",
    "\n",
    "df['Education'] = education_encoded"
   ],
   "id": "a2be3cbad82a3fb4",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "\n",
    "print('Unique Marital Status:')\n",
    "print(df['Marital_Status'].unique())\n",
    "print('\\nMarital Status value counts:')\n",
    "print(df['Marital_Status'].value_counts())"
   ],
   "id": "8efcb2cc30b29396",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Marital status evaluation\n",
    "\n",
    "### Unique Marital Status:\n",
    "`['Single' 'Together' 'Married' 'Divorced' 'Widow' 'Alone' 'Absurd' 'YOLO']\n",
    "`\n",
    "### Marital Status value counts:\n",
    "`Marital_Status,\n",
    "Married     864,\n",
    "Together    580,\n",
    "Single      480,\n",
    "Divorced    232,\n",
    "Widow        77,\n",
    "Alone         3,\n",
    "Absurd        2`\n",
    "\n",
    "## Deduction\n",
    "### Wrong data\n",
    "- Yolo alone and absurd are wrong data entries.\n",
    "- Alone is the same as single and its values can be replaced with single\n",
    "- Absurd and yolo will be replaced with the mode\n",
    "\n"
   ],
   "id": "6c76c3510b70195c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# replace Alone with Single\n",
    "df['Marital_Status'] = df['Marital_Status'].replace('Alone', 'Single')\n",
    "print('\\nMode: ',df['Marital_Status'].mode()[0])\n",
    "# Absurd and yolo  replaced with the mode\n",
    "df['Marital_Status'] = df['Marital_Status'].replace(['YOLO','Absurd'], df['Marital_Status'].mode()[0])\n",
    "\n",
    "#now check for the current state\n",
    "print('Unique Marital Status:')\n",
    "print(df['Marital_Status'].unique())\n",
    "print('\\nMarital Status value counts:')\n",
    "print(df['Marital_Status'].value_counts())\n"
   ],
   "id": "54d7b06660f856a8",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### marital status encoding\n",
    "We went with **one hot encoding**,\n",
    "this is because ~~nominal encoding~~ would create a non existent _hierachy_"
   ],
   "id": "bb62b03dddb898a8"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# One-hot encode marital status\n",
    "marital_dummies = pd.get_dummies(df['Marital_Status'], prefix='Marital')\n",
    "df = pd.concat([df.drop('Marital_Status', axis=1), marital_dummies], axis=1)\n",
    "\n",
    "print(df.info())\n"
   ],
   "id": "198cc73050c1cd14",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "\n",
    "# Feature engineering with corrected date parsing\n",
    "try:\n",
    "\n",
    "    df['Dt_Customer'] = pd.to_datetime(df['Dt_Customer'], format='mixed', dayfirst=True)\n",
    "except ValueError:\n",
    "\n",
    "    df['Dt_Customer'] = pd.to_datetime(df['Dt_Customer'], format='%d-%m-%Y', errors='coerce')\n",
    "\n",
    "\n",
    "df['Age'] = 2025- df['Year_Birth']\n",
    "df['Total_Dependents'] = df['Kidhome'] + df['Teenhome']\n",
    "df['Is_Parent'] = (df['Total_Dependents'] > 0).astype(int)\n",
    "df['Total_Spending'] = df[['MntWines','MntFruits','MntMeatProducts','MntFishProducts','MntSweetProducts','MntGoldProds']].sum(axis=1)\n",
    "df['Total_Purchases'] = df[['NumWebPurchases','NumCatalogPurchases','NumStorePurchases']].sum(axis=1)\n",
    "df['Average_Spent'] = df['Total_Spending']/df['Total_Purchases'].replace(0,1)\n",
    "df['Tenure_Days'] = (pd.to_datetime('today') - df['Dt_Customer']).dt.days\n",
    "df['TotalAcceptedCmp']=df[['AcceptedCmp1','AcceptedCmp2','AcceptedCmp3','AcceptedCmp4','AcceptedCmp5']].sum(axis=1)\n",
    "df['WebPurchaseRatio'] = df['NumWebPurchases'] / df['Total_Purchases'].replace(0, 1)\n",
    "df['StorePurchaseRatio'] = df['NumStorePurchases'] / df['Total_Purchases'].replace(0, 1)\n",
    "\n",
    "campaign_cols = ['AcceptedCmp1', 'AcceptedCmp2', 'AcceptedCmp3', 'AcceptedCmp4', 'AcceptedCmp5']\n",
    "df['Total_Accepted_Campaigns'] = df[campaign_cols].sum(axis=1)\n",
    "df['Ever_Accepted_Campaign'] = (df['Total_Accepted_Campaigns'] > 0).astype(int)\n",
    "\n",
    "# Verify date conversion\n",
    "print(\"\\nDate conversion samples:\")\n",
    "print(df[['Dt_Customer', 'Tenure_Days']].head())\n",
    "\n",
    "# Drop unnecessary columns\n",
    "# df = df.drop(['ID', 'Year_Birth', 'Z_CostContact', 'Z_Revenue','Complain','Response','Kidhome', 'Teenhome','AcceptedCmp1','MntWines','MntFruits','MntMeatProducts','MntFishProducts','MntSweetProducts','MntGoldProds','AcceptedCmp2','AcceptedCmp3','AcceptedCmp4','AcceptedCmp5','MntWines','MntFruits','MntMeatProducts','MntFishProducts','MntSweetProducts','MntGoldProds'], axis=1, errors='ignore')\n",
    "\n",
    "# Check if the featured customer CSV exists\n",
    "if os.path.exists('featured_customer_segmentation.csv'):\n",
    "    print(f\"Existing featured_customer_segmentation.csv found, removing it...\")\n",
    "    os.remove('featured_customer_segmentation.csv')\n",
    "    print(\"File removed successfully.\")\n",
    "\n",
    "# Save the new featured dataset\n",
    "df.to_csv('featured_customer_segmentation.csv', index=False)\n",
    "print(f\"New featured_customer_segmentation.csv saved successfully ({os.path.getsize('featured_customer_segmentation.csv')/1024:.2f} KB)\")\n",
    "\n"
   ],
   "id": "009e5a84",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "b3ce8e7f",
   "metadata": {},
   "source": [
    "df=pd.read_csv('cleaned_customer_segmentation.csv')\n",
    "print(\"\\nFirst 10 rows with all columns:\")\n",
    "print(df.head(10))"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "ec2da527",
   "metadata": {},
   "source": [
    "\n",
    "# Select numerical columns (exclude IDs/constants)\n",
    "numerical_cols = df.select_dtypes(include=['int64', 'float64']).columns\n",
    "cols_to_drop = ['ID','Year_Birth','Dt_Customer','Complain','Response','AcceptedCmp1','AcceptedCmp2','AcceptedCmp3','AcceptedCmp4','AcceptedCmp5']\n",
    "numerical_cols = [col for col in numerical_cols if col not in cols_to_drop]\n",
    "\n",
    "# Calculate correlations\n",
    "corr_matrix = df[numerical_cols].corr()\n",
    "\n",
    "# Set up the plot\n",
    "plt.figure(figsize=(18, 14))  # Increased size for more columns\n",
    "\n",
    "# Create heatmap with adjustments\n",
    "heatmap = sns.heatmap(\n",
    "    corr_matrix,\n",
    "    annot=True,\n",
    "    fmt=\".2f\",\n",
    "    cmap='coolwarm',\n",
    "    center=0,\n",
    "    linewidths=0.5,\n",
    "    cbar_kws={'shrink': 0.8},\n",
    "    annot_kws={'size': 10}  # Smaller annotation font\n",
    ")\n",
    "\n",
    "# Rotate and align labels\n",
    "plt.xticks(\n",
    "    rotation=45,\n",
    "    ha='right',\n",
    "    rotation_mode='anchor'\n",
    ")\n",
    "plt.yticks(rotation=0)\n",
    "\n",
    "# Force tight layout and adjust margins\n",
    "plt.tight_layout(pad=2.0)  # Extra padding\n",
    "plt.title('Correlation Heatmap (Adjusted)', fontsize=16, pad=20)\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "b012cf84",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "#this is the heatmap for the featured csv\n",
    "\n",
    "# Load data\n",
    "df = pd.read_csv('featured_customer_segmentation.csv')\n",
    "\n",
    "# Select numerical columns (exclude IDs/constants)\n",
    "numerical_cols = df.select_dtypes(include=['int64', 'float64']).columns\n",
    "cols_to_drop = ['ID','Year_Birth','Dt_Customer','Complain','Response','AcceptedCmp1','AcceptedCmp2','AcceptedCmp3','AcceptedCmp4','AcceptedCmp5']\n",
    "numerical_cols = [col for col in numerical_cols if col not in cols_to_drop]\n",
    "\n",
    "# Calculate correlations\n",
    "corr_matrix = df[numerical_cols].corr()\n",
    "\n",
    "# Set up the plot\n",
    "plt.figure(figsize=(18, 14))  # Increased size for more columns\n",
    "\n",
    "# Create heatmap with adjustments\n",
    "heatmap = sns.heatmap(\n",
    "    corr_matrix,\n",
    "    annot=True,\n",
    "    fmt=\".2f\",\n",
    "    cmap='coolwarm',\n",
    "    center=0,\n",
    "    linewidths=0.5,\n",
    "    cbar_kws={'shrink': 0.8},\n",
    "    annot_kws={'size': 10}  # Smaller annotation font\n",
    ")\n",
    "\n",
    "# Rotate and align labels\n",
    "plt.xticks(\n",
    "    rotation=45,\n",
    "    ha='right',\n",
    "    rotation_mode='anchor'\n",
    ")\n",
    "plt.yticks(rotation=0)\n",
    "\n",
    "# Force tight layout and adjust margins\n",
    "plt.tight_layout(pad=2.0)  # Extra padding\n",
    "plt.title('Correlation Heatmap (Adjusted)', fontsize=16, pad=20)\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "print(df.columns)",
   "id": "3e75eec0f0d06055",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "53572e279539dd93",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Clustering based on Spending Patterns\n",
    "In this section, we will perform clustering on the dataset to identify distinct customer segments based on their spending patterns. We will use KMeans clustering for this purpose.\n",
    "\n",
    "1. **Feature Selection**: We will select relevant features that represent customer spending patterns."
   ],
   "id": "63e936dc0b5526a0"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "spending_features = [\n",
    "    'MntWines', 'MntFruits', 'MntMeatProducts',\n",
    "    'MntFishProducts', 'MntSweetProducts', 'MntGoldProds','Total_Spending'\n",
    "]\n",
    "\n"
   ],
   "id": "3a74a0d08afdfdc9",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "2. **Data Preparation**: We will standardize the selected features to ensure that they are on the same scale.",
   "id": "424c793ac11fe0d3"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# Load dataset\n",
    "df = pd.read_csv('featured_customer_segmentation.csv')\n",
    "\n",
    "# Select relevant spending columns\n",
    "X = df[spending_features]\n",
    "\n",
    "# extract ratios for all the spending features\n",
    "# Create ratio features for spending categories, excluding the Total_Spending itself\n",
    "spending_categories = [feature for feature in spending_features if feature != 'Total_Spending']\n",
    "for feature in spending_categories:\n",
    "    X[f'{feature}_ratio'] = X[feature] / X['Total_Spending'].replace(0, 1)  # Avoid division by zero\n",
    "\n",
    "df_ratios= X[[f'{feature}_ratio' for feature in spending_categories]]\n",
    "\n",
    "\n",
    "# Scale the features\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(df_ratios)\n",
    "\n",
    "print(df_ratios.head())\n",
    "\n"
   ],
   "id": "d8958dee22644c07",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "3. Optimizing the **number of clusters** using the Elbow Method",
   "id": "96a4bb1f9c384654"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "inertia = []\n",
    "for k in range(1, 10):\n",
    "    kmeans = KMeans(n_clusters=k, random_state=42)\n",
    "    kmeans.fit(X_scaled)\n",
    "    inertia.append(kmeans.inertia_)\n",
    "\n",
    "print(inertia)\n",
    "\n",
    "plt.plot(range(1, 10), inertia, marker='o')\n",
    "plt.xlabel('Number of Clusters')\n",
    "plt.ylabel('Inertia')\n",
    "plt.title('Elbow Method for Spending Pattern Clustering')\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ],
   "id": "f7270e3ee38cef8e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "4. **Applying KMeans Clustering** with the optimal number of clusters\n",
    "We will choose 3 clusters based on the elbow method.\n",
    "this is because the inertia starts to level off after 3 clusters, indicating that adding more clusters does not significantly reduce inertia."
   ],
   "id": "da29896a93c41851"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Apply KMeans clustering with clusters\n",
    "kmeans = KMeans(n_clusters=3, random_state=42)\n",
    "df['Spending_Cluster'] = kmeans.fit_predict(X_scaled)\n",
    "\n",
    "\n",
    "cluster_summary = df.groupby('Spending_Cluster')[spending_features].mean().round(2)\n",
    "print(cluster_summary)\n"
   ],
   "id": "e79c73149542b575",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "5. **Analyzing Cluster Characteristics**\n",
    "\n",
    "We will analyze the characteristics of each cluster by computing the mean spending for each product category.\n",
    "\n",
    "We will also visualize the average spending per product category for each cluster using a grouped bar chart.\n"
   ],
   "id": "1ac8bf10aa7eb5b"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "5c742b0617cc8d27"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Group by cluster and compute means\n",
    "cluster_means = df.groupby('Spending_Cluster')[spending_features].mean()\n",
    "\n",
    "print(cluster_means)\n",
    "\n",
    "# Transpose for easier plotting (products on x-axis)\n",
    "cluster_means_T = cluster_means.T\n",
    "\n",
    "# Plot grouped bar chart\n",
    "cluster_means_T.plot(kind='bar', figsize=(10, 10))\n",
    "\n",
    "plt.title('Average Spending per Product Category by Cluster')\n",
    "plt.xlabel('Product Category')\n",
    "plt.ylabel('Average Spending')\n",
    "plt.legend(title='Cluster')\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "id": "9385ceb66e10dfd4",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Here is the grouped bar chart showing the average spending per product category for each cluster. Each bar represents the average spending for a specific product category, and different colors represent different clusters. This visualization helps us understand how each cluster allocates its spending across various product categories.",
   "id": "577587a538ba362"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Transpose the DataFrame so categories are on the x-axis\n",
    "cluster_means.T.plot(kind='bar', figsize=(10, 6), colormap='Set1')\n",
    "\n",
    "plt.title('Average Spending per Product Category by Cluster')\n",
    "plt.ylabel('Average Amount Spent')\n",
    "plt.xlabel('Product Category')\n",
    "plt.xticks(rotation=45)\n",
    "plt.legend(title='Cluster')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ],
   "id": "5b4e93479e8043ec",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "This is a stacked bar chart showing the average spending per product category for each cluster. Each segment of the bar represents the average spending for a specific product category, and different colors represent different clusters. This visualization helps us understand how each cluster allocates its spending across various product categories.",
   "id": "a6b75e366b3f5535"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Normalize category spending by total spending per cluster\n",
    "normalized = df.groupby('Spending_Cluster')[\n",
    "    ['MntWines', 'MntFruits', 'MntMeatProducts', 'MntFishProducts',\n",
    "     'MntSweetProducts', 'MntGoldProds']\n",
    "].mean()\n",
    "\n",
    "# Add total spending per cluster\n",
    "normalized['Total'] = normalized.sum(axis=1)\n",
    "\n",
    "# Divide each category by total to get proportions\n",
    "normalized_pct = normalized.div(normalized['Total'], axis=0).drop(columns='Total')\n",
    "\n",
    "# Optional: Multiply by 100 for percentage\n",
    "normalized_pct *= 100\n",
    "\n",
    "print(normalized_pct.round(2))\n",
    "\n",
    "# plot the normalized dta\n",
    "# Create a colorful stacked bar chart with improved styling\n",
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "# Use a colorful palette\n",
    "colors = ['#ff9999', '#66b3ff', '#99ff99', '#ffcc99', '#c2c2f0', '#ffb3e6']\n",
    "normalized_pct.plot(\n",
    "    kind='bar',\n",
    "    stacked=True,  # Use stacked bars to show composition\n",
    "    figsize=(12, 8),\n",
    "    color=colors,\n",
    "    width=0.7,  # Slightly thinner bars\n",
    "    edgecolor='white'  # White edge for contrast between segments\n",
    ")\n",
    "\n",
    "# Enhance styling\n",
    "plt.title('Spending Composition by Customer Segment', fontsize=16, pad=20)\n",
    "plt.xlabel('Customer Segment', fontsize=14, labelpad=10)\n",
    "plt.ylabel('Percentage of Total Spending (%)', fontsize=14, labelpad=10)\n",
    "plt.xticks(rotation=0, fontsize=12)\n",
    "plt.yticks(fontsize=12)\n",
    "\n",
    "# Add percentage signs to y-axis\n",
    "current_values = plt.gca().get_yticks()\n",
    "plt.gca().set_yticklabels(['{:.0f}%'.format(x) for x in current_values])\n",
    "\n",
    "# Improve legend\n",
    "plt.legend(\n",
    "    title='Product Category',\n",
    "    title_fontsize=12,\n",
    "    fontsize=10,\n",
    "    loc='upper right',\n",
    "    bbox_to_anchor=(1.15, 1)\n",
    ")\n",
    "\n",
    "# Add value labels on the stacked bars\n",
    "for i, cluster in enumerate(normalized_pct.index):\n",
    "    cumulative = 0\n",
    "    for j, col in enumerate(normalized_pct.columns):\n",
    "        value = normalized_pct.loc[cluster, col]\n",
    "        if value > 5:  # Only show labels for segments > 5%\n",
    "            plt.text(\n",
    "                i,\n",
    "                cumulative + value / 2,  # Center of segment\n",
    "                f\"{value:.1f}%\",\n",
    "                ha='center',\n",
    "                va='center',\n",
    "                fontsize=9,\n",
    "                fontweight='bold',\n",
    "                color='black' if value > 15 else 'white'  # Better contrast\n",
    "            )\n",
    "        cumulative += value\n",
    "\n",
    "# Add grid lines for better readability\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "\n",
    "# Rename x-axis labels to more meaningful names\n",
    "plt.gca().set_xticklabels([\n",
    "    'Segment 0',\n",
    "    'Segment 1',\n",
    "    'Segment 2'\n",
    "])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ],
   "id": "67cda1ccb55c7122",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
